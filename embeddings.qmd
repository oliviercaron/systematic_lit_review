---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "An embeddings analysis"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Libraries and loading data

```{r}
#| label: load-packages
#| message: false

library(tidyverse)
library(word2vec)
library(quanteda)
library(text)
library(udpipe)
library(text2vec)
```

## Loading data

```{r}
#| label: load-data


list_articles <- read.csv2("nlp_full_data_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("entry_number" = 1)
list_references <- read.csv2("nlp_references_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("citing_art" = 1)
colnames(list_articles) <- gsub("\\.+", "_", colnames(list_articles)) # <1>
colnames(list_articles) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_articles)) # <2>
colnames(list_references) <- gsub("\\.+", "_", colnames(list_references))
colnames(list_references) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_references))

data_embeddings <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE) %>%
  filter(marketing == 1) %>%
  mutate("combined_text" = paste0(dc_title,". ", dc_description)) %>%
  mutate("year" = substr(prism_coverDate, 7, 10))
```

## A glimpse of data

```{r}
#| label: glimpse-data

data_embeddings %>%
  head(5) %>%
  select(entry_number, dc_creator, combined_text, year)
```

## A first Word2Vec embeddings analysis (skip-gram)

```{r}
#| label: word2vec

set.seed(42)
model <- word2vec(x = tolower(data_embeddings$combined_text), type = "skip-gram", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(model)


```

```{r}
lookslike <- predict(model, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## A Word2Vec embeddings analysis (cbow)

```{r}
#| label: word2vec

set.seed(42)
modelcbow <- word2vec(x = tolower(data_embeddings$combined_text), type = "cbow", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(modelcbow)
#embedding <- predict(modelcbow, c("mining"), type = "embedding")
#embedding
```

```{r}
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Part of speech tagging with UDPipe

```{r}
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
```

## Top 20 nouns with UDPipe

```{r}

lemma <- annotated_text %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Part of speech tagging with trankit

```{python}
from trankit import Pipeline
# initialize a pipeline for English
p = Pipeline('english')

# a non-empty string to process, which can be a document or a paragraph with multiple sentences
doc_text = '''Hello! This is Trankit.'''

all = p.posdep(doc_text)
```

```{python}
from trankit import Pipeline

p = Pipeline(lang='english', gpu=True, cache_dir='./cache')
```

## 

## Part of speech tagging with stanza

```{python}
import stanza
import json
import pandas as pd

nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
doc = nlp('Barack Obama was born in Hawaii.')
#print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
print(doc)

#transform stanza document into dic
dicts = doc.to_dict()

dicts 

#transform dict into dataframe
df = pd.DataFrame(dicts[0])
df

df.to_csv("testdf.csv")
```

```{r}
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## A global vector embeddings (GloVe) analysis

```{r}
library(text2vec)
text8_file = "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~/")
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
head(wiki)

https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html
```

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(data_embeddings$combined_text)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab
```
