---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "An embeddings analysis"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Libraries and loading data

```{r}
#| label: load-packages
#| message: false

library(tidyverse)
library(word2vec)
library(quanteda)
library(text)
library(udpipe)
library(text2vec)
```

## Loading data

```{r}
#| label: load-data


list_articles <- read.csv2("nlp_full_data_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("entry_number" = 1)
list_references <- read.csv2("nlp_references_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("citing_art" = 1)
colnames(list_articles) <- gsub("\\.+", "_", colnames(list_articles)) # <1>
colnames(list_articles) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_articles)) # <2>
colnames(list_references) <- gsub("\\.+", "_", colnames(list_references))
colnames(list_references) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_references))


data_embeddings <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE) %>%
  filter(marketing == 1) %>%
  mutate("year" = substr(prism_coverDate, 7, 10)) %>%
  mutate(keywords = str_replace_all(authkeywords, "\\|", "")) %>%
  mutate(keywords = str_squish(keywords)) %>%
  mutate("combined_text" = paste0(dc_title,". ", dc_description, ". ", keywords))

write.csv(data_embeddings,"data_for_embeddings.csv")
data_embeddings <- read.csv("data_for_embeddings.csv")
embeddings <- read.csv("embeddings_bge.csv")
```

## A glimpse of data

```{r}
#| label: glimpse-data

data_embeddings %>%
  head(5) %>%
  select(entry_number, dc_creator, combined_text, year)
```

## A first Word2Vec embeddings analysis (skip-gram)

```{r}
#| label: word2vec

set.seed(42)
model <- word2vec(x = tolower(data_embeddings$combined_text), type = "skip-gram", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(model)


```

```{r}
lookslike <- predict(model, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## A Word2Vec embeddings analysis (cbow)

```{r}
#| label: word2vec

set.seed(42)
modelcbow <- word2vec(x = tolower(data_embeddings$combined_text), type = "cbow", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(modelcbow)
#embedding <- predict(modelcbow, c("mining"), type = "embedding")
#embedding
```

```{r}
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Part of speech tagging with UDPipe

```{r}
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
write.csv(annotated_text,"annotated_udpipe.csv")
```

## Top 20 nouns with UDPipe

```{r}

lemma <- annotated_text %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Part of speech tagging with Trankit (@nguyen2021trankit)

```{python}
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize

df = pd.read_csv("data_for_embeddings.csv")

# initialize a pipeline for English
p = Pipeline('english')

#test = p.posdep(df.loc[:5]['combined_text'][1])
#testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
  
#pos = p.posdep(all)


results = pd.DataFrame()  

for text in df['combined_text']:
    pos = p.posdep(text)
    pos_df = pd.json_normalize(pos['sentences'], 'tokens')
    results = pd.concat([results,pos_df])


results.to_csv("annotated_trankit.csv")
```

## Part of speech tagging with Stanza (@qi2020stanza)

```{python}
import stanza
import pandas as pd
from tqdm import tqdm

# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)

# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")


annotated_df = pd.DataFrame()

for text in tqdm(df['combined_text'], desc="Processing Texts"):
    doc = nlp(text)
    dicts = doc.to_dict()
    
    # Convertissez le dictionnaire en un DataFrame temporaire
    temp_df = pd.DataFrame(dicts[0])
    
    # Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
    annotated_df = pd.concat([annotated_df, temp_df], ignore_index=True)



annotated_df.to_csv("annotated_stanza.csv")

```

## Top 20 nouns with Stanza

```{r}
annotation_stanza <- read.csv("annotated_stanza.csv")
lemma <- annotation_stanza %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))
```

## **Custom Embeddings for BERTopic**

```{python}
from sklearn.datasets import fetch_20newsgroups
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas

vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")

# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")

docs_marketing = df["combined_text"].tolist()


sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs_marketing, show_progress_bar=True)

# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)

# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs_marketing, embeddings)
```

```{python}

topic_model.get_topic_info()

len(topic_model.get_topic(-1)
```

## Another model of embeddings (bge-large-en)

```{python}

model = SentenceTransformer('BAAI/bge-large-zh')
embeddings_1 = model.encode(sentences, normalize_embeddings=True)
embeddings_2 = model.encode(sentences, normalize_embeddings=True)
similarity = embeddings_1 @ embeddings_2.T
print(similarity)


sentence_model = SentenceTransformer('BAAI/bge-large-zh')
embeddings = sentence_model.encode(docs_marketing, show_progress_bar=True, normalize_embeddings=True)

# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model_bge = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)

# Ajustez le modèle BERTopic aux embeddings
topics_bge, probs_bge = topic_model_bge.fit_transform(docs_marketing, embeddings)
```

```{python}
topic_model.get_topic_info()

topic_model.get_topic(3)

fig_topics_bge = topic_model.visualize_topics()
fig_topics_bge.write_html("topics_bge.html")

fig_hierarchy_bge = topic_model.visualize_hierarchy()
fig_hierarchy_bge.write_html("hierarchy_bge.html")

hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
fig_hierarchical_topics_bge = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics_bge.write_html("hierarchical_topics_bge.html")

fig_barchart_bge = topic_model.visualize_barchart()
fig_barchart_bge.write_html("barchart_bge.html")

fig_heatmap_bge = topic_model.visualize_heatmap()
fig_heatmap_bge.write_html("fig_heatmap_bge.html")

```

## Save BERTopic model

```{python}
# Method 1 - safetensors
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 2 - pytorch
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="pytorch", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 3 - pickle
topic_model.save("my_model", serialization="pickle")


#save my topic model
embedding_model = "BAAI/bge-large-zh"
topic_model.save("embeddings_bge_model",serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)


#save my embeddings model
embeddings_df = pd.DataFrame(embeddings)
embeddings_df.to_csv("embeddings_bge.csv", index=False)
```
