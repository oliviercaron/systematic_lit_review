---
pip ---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "An embeddings analysis"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Libraries and loading data

```{r}
#| label: load-packages
#| message: false

library(tidyverse)
library(word2vec)
library(quanteda)
library(text)
library(udpipe)
library(text2vec)
```

## Loading data

```{r}
#| label: load-data


list_articles <- read.csv2("nlp_full_data_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("entry_number" = 1)
list_references <- read.csv2("nlp_references_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("citing_art" = 1)
colnames(list_articles) <- gsub("\\.+", "_", colnames(list_articles)) # <1>
colnames(list_articles) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_articles)) # <2>
colnames(list_references) <- gsub("\\.+", "_", colnames(list_references))
colnames(list_references) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_references))


data_embeddings <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE) %>%
  filter(marketing == 1) %>%
  mutate("year" = substr(prism_coverDate, 7, 10)) %>%
  mutate(keywords = str_replace_all(authkeywords, "\\|", "")) %>%
  mutate(keywords = str_squish(keywords)) %>%
  mutate("combined_text" = paste0(dc_title,". ", dc_description, ". ", keywords))

#write.csv(data_embeddings,"data_for_embeddings.csv")
#data_embeddings <- read.csv("data_for_embeddings.csv")
#embeddings <- read.csv("embeddings_bge.csv")
```

## A glimpse of data

```{r}
#| label: glimpse-data

data_embeddings %>%
  head(5) %>%
  select(entry_number, dc_creator, combined_text, year)
```

## A first Word2Vec embeddings analysis (skip-gram)

```{r}
#| label: word2vec

set.seed(42)
model <- word2vec(x = tolower(data_embeddings$combined_text), type = "skip-gram", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(model)


```

```{r}
lookslike <- predict(model, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## A Word2Vec embeddings analysis (cbow)

```{r}
#| label: word2vec

set.seed(42)
modelcbow <- word2vec(x = tolower(data_embeddings$combined_text), type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
embedding1 <- as.matrix(modelcbow)
#embedding <- predict(modelcbow, c("mining"), type = "embedding")
#embedding
```

```{r}
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Part of speech tagging with UDPipe (@straka-strakova-2017-tokenizing)

```{r}
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
write.csv(annotated_text,"annotated_udpipe.csv")
```

## Top 20 nouns with UDPipe

```{r}

lemma <- annotated_text %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Part of speech tagging with Trankit (@nguyen2021trankit)

```{python}
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize

df = pd.read_csv("data_for_embeddings.csv")

# initialize a pipeline for English
p = Pipeline('english')

#test = p.posdep(df.loc[:5]['combined_text'][1])
#testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
  
#pos = p.posdep(all)

results = pd.DataFrame()  
#part of speech tagging
for text in df['combined_text']:
    pos = p.posdep(text)
    pos_df = pd.json_normalize(pos['sentences'], 'tokens')
    results = pd.concat([results,pos_df])

#lemmatization
results_lemma = pd.DataFrame()  
for text in df['combined_text']:
    lemma = p.lemmatize(text)
    lemma_df = pd.json_normalize(lemma['sentences'], 'tokens')
    results_lemma = pd.concat([results_lemma,lemma_df])
    
#join both data frames
results_complete = pd.concat([results, results_lemma['text'].rename("lemma")], axis=1)
results_complete["lemma"] = results_complete["text"]
results_lemma.to_csv("lemmas_trankit.csv")
results_complete.to_csv("annotated_trankit.csv")
```

## Top 20 nouns with Trankit

```{python}
import pandas as pd
import plotly.express as px

# Charger les données à partir du DataFrame "results"
# Assurez-vous que "results" contient les mêmes colonnes que le fichier CSV original
# (par exemple, "upos" et "lemma")
results_complete = pd.read_csv("annotated_trankit.csv")

# Filtrer les lignes où 'upos' est égal à "NOUN"
noun_data = results_complete[results_complete['upos'] == 'NOUN']

# Regrouper par 'lemma' et compter le nombre d'occurrences
top_nouns = noun_data['lemma'].value_counts().reset_index()
top_nouns.columns = ['lemma', 'n']
top_nouns = top_nouns.head(20)

# Créer un graphique à l'aide de Plotly Express
fig = px.scatter(top_nouns, x='n', y='lemma', color='lemma',
                 labels={'n': 'Frequency', 'lemma': 'Lemma'},
                 title='Top 20 Most Frequent Nouns')

# Personnaliser le style du graphique
fig.update_traces(marker=dict(size=12, opacity=0.6),
                  selector=dict(mode='markers'))

fig.update_layout(title_x=0.5, title_font=dict(size=20))
fig.update_layout(template="plotly_white")


# Afficher le graphique
fig.show()
fig.write_html("top_20_nouns_trankit_python.html")



```

## Part of speech tagging with Stanza (@qi2020stanza)

```{python}
import stanza
import pandas as pd
from tqdm import tqdm

# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)

# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")


annotated_df = pd.DataFrame()

for text in tqdm(df['combined_text'], desc="Processing Texts"):
    doc = nlp(text)
    dicts = doc.to_dict()
    
    # Convertissez le dictionnaire en un DataFrame temporaire
    temp_df = pd.DataFrame(dicts[0])
    
    # Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
    annotated_df = pd.concat([annotated_df, temp_df], ignore_index=True)



annotated_df.to_csv("annotated_stanza.csv")

```

## Top 20 nouns with Stanza

```{r}
annotation_stanza <- read.csv("annotated_stanza.csv")
lemma <- annotation_stanza %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))
```

## **Python libraries and loading data**

```{python}
import warnings
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")

from tqdm import tqdm
from transformers import XLNetTokenizer, XLNetModel
from sentence_transformers import SentenceTransformer, util
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples
from yellowbrick.cluster import SilhouetteVisualizer
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

import nltk
import numpy as np
import pandas as pd
import hdbscan
import plotly.express as px
import plotly.io as pio
import seaborn as sns
import matplotlib.pyplot as plt
import time
import umap.umap_ as umap
import torch


df = pd.read_csv("data_for_embeddings.csv")
df['title_abstract'] = df['dc_title'].astype(str) + '. ' + df['dc_description'].astype(str)
docs_marketing = df["combined_text"].tolist()
```

## CUDA Status and Device Info

```{python}
print(f"Is CUDA supported by this system? {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")

# Storing ID of the current CUDA device
cuda_id = torch.cuda.current_device()
print(f"ID of the current CUDA device: {cuda_id}")

print(f"Name of the current CUDA device: {torch.cuda.get_device_name(cuda_id)}")
```

## Detect interpretable topics with BERTopic

::: callout-note
We use a CountVectorizer which enables us to specify the range of the ngram we want in our topic model. We can use it before or after the topic modelling (update topic).\
Here we use it before the topic modelling to exclude english stopwords, but after the embeddings process so that the foundation provided by stopwords in sentences is preserved in context.
:::

### **Function to create BERTopics with custom embeddings**

The aim of this function is to swiftly create various BERTopic experiments while maintaining the same parameters, except for the choice of the embedding model. This enables the generation of distinct BERTopic results, facilitating meaningful comparisons among them.

Some explanations:

| Parameter name   | Description                                                                                                                                                                                                                |
|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| docs             | The documents we want to analyze (list).                                                                                                                                                                                   |
| embeddings_model | Specifies the embeddings model we want to load and use.                                                                                                                                                                    |
| min_topic_size   | It is used to specify what the minimum size of a topic can be. See [BERTopic documentation](https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#min_topic_size "min_topic_size"). |
| nr_topics        | The number of topics we want to reduce our results to. See [BERTopic documentation](https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#nr_topics "min_topic_size").              |

```{python}

def create_bertopic(docs, embeddings_model, min_topic_size, nr_topics):
  
    # initialize a count-based tf-idf transformer
    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)

    # initialize a sentence transformer model for embeddings
    sentence_model = SentenceTransformer(embeddings_model, device='cuda')

    # generate embeddings for the input documents
    embeddings = sentence_model.encode(docs, show_progress_bar=True)

    # create a bertopic model with specified parameters
    topic_model = BERTopic(
        ctfidf_model=ctfidf_model,
        calculate_probabilities=True,
        verbose=True,
        min_topic_size=min_topic_size,
        nr_topics=nr_topics
    )

    # fit the bertopic model to the input documents and embeddings
    topics, probs = topic_model.fit_transform(docs, embeddings)

    # update the vectorizer model used by bertopic
    updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3), min_df=0.05)
    topic_model.update_topics(docs, vectorizer_model=updated_vectorizer_model)

    # return the trained bertopic model
    return topic_model

  
topic_model_test = create_bertopic(docs_marketing, "all-mpnet-base-v2", 5, 13)

model_name = "test"
visualize_bertopic(topic_model_test, model_name)
```

### Function to visualize BERTopics

Creates a folder in Images/ with the model_name input with various plots in html files.

```{python}
import os
from tabulate import tabulate

def generate_topics_table(topic_model):
    # get topic information from the model
    topics_info = topic_model.get_topic_info()

    # convert the data into a list
    data_as_list = topics_info.values.tolist()

    # get column names as headers
    headers = topics_info.columns.tolist()

    # generate the table in HTML format
    table = tabulate(data_as_list, headers, tablefmt='html')

    # save the table to an HTML file
    with open('table_topics.html', 'w') as f:
        f.write(table)
        
        

def visualize_bertopic(topic_model, model_name):
    # create the "images" folder if it doesn't exist already
    if not os.path.exists("images"):
        os.makedirs("images")

    # create a subfolder for the specific topic model
    model_folder = os.path.join("images", model_name)
    
    # Create the model folder if it doesn't exist
    if not os.path.exists(model_folder):
        os.makedirs(model_folder)
    else:
        # Delete existing files in the model folder
        for file in os.listdir(model_folder):
            os.remove(os.path.join(model_folder, file))

    # generate topics information table
    generate_topics_table(topic_model)
    os.rename('table_topics.html', os.path.join(model_folder, 'table_topics.html'))

    # visualize topics
    fig_topics = topic_model.visualize_topics()
    fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))

    # visualize hierarchy
    fig_hierarchy = topic_model.visualize_hierarchy()
    fig_hierarchy.write_html(os.path.join(model_folder, "hierarchy.html"))

    # visualize hierarchical topics
    hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
    fig_hierarchical_topics = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
    fig_hierarchical_topics.write_html(os.path.join(model_folder, "hierarchical.html"))

    # visualize the bar chart
    fig_barchart = topic_model.visualize_barchart(width=300, height=300, n_words=10, topics=None, top_n_topics=20)
    fig_barchart.write_html(os.path.join(model_folder, "barchart.html"))

    # visualize the heatmap
    fig_heatmap = topic_model.visualize_heatmap()
    fig_heatmap.write_html(os.path.join(model_folder, "heatmap.html"))
    
    # topics over time
    years =  df['year'].to_list()
    topics_over_time = topic_model.topics_over_time(docs_marketing, years)
    fig_topics_over_time = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, normalize_frequency=True)
    fig_topics_over_time.write_html(os.path.join(model_folder, "topicsovertime.html"))
    
    # visualize the documents
    fig_documents = topic_model.visualize_documents(docs_marketing, embeddings=embeddings_mpnet)
    fig_documents.write_html(os.path.join(model_folder, "documents.html"))
```

### Create topics for a list of embeddings model

```{python}

# list of embeddings models
list_embeddings = ["all-mpnet-base-v2", "all-mpnet-base-v1"]

# create a list to store model information
table_data = []
topic_models = {}

# loop through the list of embeddings models
for embeddings_model in list_embeddings:
    # replace these values with actual dimensions and max tokens
    print(f"\nCreating BERTopics with the {embeddings_model} Sentence-Transformers pretrained model.")
    topic_model = create_bertopic(docs_marketing, embeddings_model, 5, 13)
    visualize_bertopic(topic_model, embeddings_model)
    chargedmodel = SentenceTransformer(embeddings_model, device='cuda')
    dimensions =  chargedmodel.get_sentence_embedding_dimension()
    max_tokens = chargedmodel.max_seq_length
    # Store the topic_model in the dictionary with the embeddings name as key
    topic_models[embeddings_model] = topic_model
    # add model information to the table data list
    table_data.append([embeddings_model, dimensions, max_tokens])


# table headers
headers = ["Embeddings Model", "Dimensions", "Max Tokens"]

# title for the table, centered
table_title = "Summary of Embeddings Models used"

# create the table with centered title
table = tabulate(table_data, headers, tablefmt="pretty")
table_lines = table.split("\n")
table_lines.insert(0, table_title.center(len(table_lines[0])))
table_with_centered_title = "\n".join(table_lines)

# display the table with centered title
print("\n")
print(table_with_centered_title)

#sentence_model.max_seq_length
#sentence_model.get_sentence_embedding_dimension()
```

```{python}
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

docs_marketing[0]
summarizer(docs_marketing[0])
```

```{python}

# create a countvectorizer object with specified settings:
# - ngram_range=(1, 3): generate both unigrams, bigrams and trigrams from the text.
#   this means it will consider individual words as well as pairs of consecutive words.
# - stop_words="english": use a predefined list of English stop words to exclude common words like "the," "and," etc.

#vectorizer_model = CountVectorizer(ngram_range=(1, 3)) #, stop_words="english"
# reduce the impact of frequent words
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)

# create a sentence model using the "all-mpnet-base-v2" pre-trained model 
sentence_model = SentenceTransformer("all-mpnet-base-v2", device='cuda')

# encode the 'docs_marketing' text data into embeddings
# and show a progress bar while processing
embeddings_mpnet = sentence_model.encode(docs_marketing, show_progress_bar=True)

# create an instance of BERToopic with vectorizer_model and other parameters
topic_model_mpnet = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)

# adjust BERTopic to embeddings. It extracts topics and corresponding probabilities.
topics_mpnet, probs_mpnet = topic_model_mpnet.fit_transform(docs_marketing, embeddings_mpnet)

#update topic
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_mpnet.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)

#for sentence, embedding in zip(docs_marketing, embeddings_mpnet):
    #print("Sentence:", sentence)
    #print("Embedding:", embedding)
    #print("")

```

## 

## **BERTopic** with **Custom Embeddings model** (xlnet) (@DBLP:journals/corr/abs-1906-08237)

```{python}

# check if cuda is available
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f'using gpu: {torch.cuda.get_device_name(0)}')
else:
    device = torch.device('cpu')
    print('cuda is not available, using cpu.')

# load xlnet tokenizer and model on gpu
tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')
model = XLNetModel.from_pretrained('xlnet-large-cased').to(device)

# list of phrases
phrases = docs_marketing

# create a list to store the embeddings
embeddings_list = []

# use tqdm to create a progress bar
for phrase in tqdm(phrases, desc="calculating embeddings"):
    # tokenize the phrase
    inputs = tokenizer(phrase, return_tensors="pt").to(device)
    
    # get embeddings
    with torch.no_grad():
        outputs = model(**inputs)
    
    # retrieve the last hidden state
    last_hidden_states = outputs.last_hidden_state
    
    # calculate the average of embeddings for each phrase
    average_embedding = last_hidden_states.mean(dim=1).cpu().numpy()
    
    # add the embedding to the list
    embeddings_list.append(average_embedding)

# flatten the list of embeddings
embeddings_array = np.concatenate(embeddings_list)

# if we want a dataframe, we can use the following commands:

flat_embeddings = [embedding[0] for embedding in embeddings_list]

# create a dataframe from the list of embeddings
dftest = pd.DataFrame(flat_embeddings)

# display the dataframe
#print(dftest)


topic_model_xlnet = BERTopic(vectorizer_model=vectorizer_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=14)

topics_xlnet, probs_xlnet = topic_model_xlnet.fit_transform(docs_marketing, embeddings_array)


```

## 

## **BERTopic** with **Custom Embeddings model** (Word2Vec)

```{python}

# Assurez-vous d'avoir téléchargé les données nécessaires pour NLTK (une seule fois)
nltk.download('punkt')


# Tokenisation des phrases et entraînement du modèle Word2Vec
vector_size = 100  # Taille des vecteurs d'embedding
window_size = 7    # Fenêtre de contexte
min_count = 50      # Ignorer les mots ayant une fréquence inférieure à min_count
sg = 0             # CBOW (skip-gram si sg=1)

tokenized_docs_marketing = [word_tokenize(sentence) for sentence in docs_marketing]

model = Word2Vec(tokenized_docs_marketing, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg)

# Maintenant, vous pouvez accéder aux embeddings de mots, par exemple :
embedding_machine = model.wv['machine']
print(embedding_machine)


```

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Obtenez les embeddings de tous les mots dans le modèle Word2Vec
embeddings = [model.wv[word] for word in model.wv.index_to_key]

# Convertissez la liste d'embeddings en un tableau NumPy
embeddings_array = np.array(embeddings)

# Utilisez PCA pour réduire la dimension à 2D
pca = PCA(n_components=2)
pca_result = pca.fit_transform(embeddings_array)

# Utilisez t-SNE pour une réduction de dimension non linéaire à 2D
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
tsne_result = tsne.fit_transform(embeddings_array)

# Visualisation avec PCA
plt.figure(figsize=(10, 5))
plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)
for i, word in enumerate(model.wv.index_to_key):
    plt.annotate(word, xy=(pca_result[i, 0], pca_result[i, 1]), alpha=0.7)
plt.title('2D projection of word embeddings (t-SNE)')
plt.show()

# Visualisation avec t-SNE
import plotly.express as px

# Créez un DataFrame Pandas pour stocker les résultats de t-SNE
import pandas as pd
tsne_df = pd.DataFrame(tsne_result, columns=['Dimension 1', 'Dimension 2'])
tsne_df['Word'] = model.wv.index_to_key

# Créez un nuage de points interactif avec Plotly
fig = px.scatter(tsne_df, x='Dimension 1', y='Dimension 2', text='Word', title='Projection en 2D des embeddings de mots (t-SNE)')
fig.update_traces(textposition='top center', textfont_size=10, marker=dict(size=8, opacity=0.5))
fig.show()


```

```{python}
from sklearn.metrics.pairwise import cosine_similarity
#test similarity
query_embedding = sentence_model.encode('Cats are not mammals.')
passage_embedding = sentence_model.encode('Cats are mammals.')
similarity = query_embedding @ passage_embedding
print(similarity)
print("Similarity:", util.dot_score(query_embedding, passage_embedding))

similarity_cosine = cosine_similarity([query_embedding], [passage_embedding])
print("Similarity (Cosine):", similarity_cosine[0][0])


```

## TEST TEST TEST WORD2VEC PLOT TEST TEST TEST

## 

## Visualize how each token contributes to a specific topic.

We can do it by selecting a sentence in the document. To do so, we need to first calculate topic distributions on a token level and then visualize the results:\
More parameters here :\
https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html

```{python}
# Calculate the topic distributions on a token-level, we can add the window
topic_distr, topic_token_distr = topic_model_bge.approximate_distribution(docs_marketing, calculate_tokens=True)

# Visualize the token-level distributions, put id of document
df_topicmodel = topic_model_bge.visualize_approximate_distribution(docs_marketing[150], topic_token_distr[150])
df_topicmodel

topics_html = df_topicmodel.to_html()

topics_html

with open('topics_html.html', 'w') as html_file:
    html_file.write(topics_html)
```

### Test visualize_documents

```{python}
fig_documents = topic_model_mpnet.visualize_documents(docs_marketing, embeddings=embeddings_mpnet)
fig_documents.write_html("test_viz_documents.html")

 fig_topics = topic_model.visualize_topics()
    fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))

```

## Save BERTopic model

```{python}
# Method 1 - safetensors
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 2 - pytorch
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="pytorch", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 3 - pickle
topic_model.save("topic_model_xlnet", serialization="pickle")

#save my topic model
embedding_model = "BAAI/bge-large-zh"
topic_model.save("topic_model_xlnet",serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)


#save my embeddings model
embeddings_df = pd.DataFrame(embeddings)
embeddings_df.to_csv("embeddings_bge.csv", index=False)
```

## Plotting authors and text based on embeddings

### 3D plot of authors and year with PCA as dimensional reduction technique

```{python}

# Étape 1 : Réduction des dimensions avec PCA
pca = PCA(n_components=3)
embeddings_3D = pca.fit_transform(embeddings_bge)


# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_3D[:, 0]
df['Y'] = embeddings_3D[:, 1]
df['Z'] = embeddings_3D[:, 2]

print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))


# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_3D)

#evaluate clustering
davies_bouldin_score(embeddings_3D, df['Cluster'])
silhouette_score(embeddings_3D, df['Cluster'])

    
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'Z', 'dc_creator', 'year', 'Cluster']]

# Étape 5 : Création du graphique 3D interactif avec Plotly
fig = px.scatter_3d(df_vis, x='X', y='Y', z='Z', color='Cluster', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 3D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()

pio.write_html(fig, file="3D_embeddings_PCA.html")

```

### 3D plot of authors and year with TSNE as dimensional reduction technique

```{python}

# Étape 1 : Réduction des dimensions avec t-SNE
from sklearn.manifold import TSNE

tsne = TSNE(n_components=3)
embeddings_3D = tsne.fit_transform(embeddings_bge)

# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_3D[:, 0]
df['Y'] = embeddings_3D[:, 1]
df['Z'] = embeddings_3D[:, 2]

# Étape 3 : Clustering avec HDBSCAN
import hdbscan
from sklearn.metrics import davies_bouldin_score, silhouette_score

clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster_tsne'] = clusterer.fit_predict(embeddings_3D)

# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_3D, df['Cluster_tsne'])
silhouette = silhouette_score(embeddings_3D, df['Cluster_tsne'])

print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)

# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'Z', 'dc_creator', 'year', 'Cluster_tsne']]

# Étape 5 : Création du graphique 3D interactif avec Plotly
import plotly.express as px
import plotly.io as pio

fig = px.scatter_3d(df_vis, x='X', y='Y', z='Z', color='Cluster_tsne', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE and Clustering 3D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()

pio.write_html(fig, file="3D_embeddings_TSNE.html")

```

### 2D plot of authors and year with PCA as dimensional reduction technique

```{python}

df = df[df['citedby_count'] > 20]
doc_marketing_sup20citations = df['combined_text'].to_list()

sentence_model_bge = SentenceTransformer('BAAI/bge-large-en', device='cuda')
embeddings_bge_sup20citations = sentence_model_bge.encode(doc_marketing_sup20citations, show_progress_bar=True, normalize_embeddings=True)

pca = PCA(n_components=50)
embeddings_2D = pca.fit_transform(embeddings_bge_sup20citations)
print ("Proportion of Variance Explained : ", pca.explained_variance_ratio_)
    
out_sum = np.cumsum(pca.explained_variance_ratio_)  
print ("Cumulative Prop. Variance Explained: ", out_sum)

type(embeddings_2D)
type(doc_marketing_sup20citations)
doc_marketing_sup20citations = pd.DataFrame(doc_marketing_sup20citations, columns=["combined_text"])  # Replace "Column1", "Column2", ... with actual column names

# Étape 2 : Coller les embeddings réduits au DataFrame df
doc_marketing_sup20citations['X'] = embeddings_2D[:, 0]
doc_marketing_sup20citations['Y'] = embeddings_2D[:, 1]

print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN( metric='euclidean')
doc_marketing_sup20citations['Cluster'] = clusterer.fit_predict(embeddings_2D)

# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, doc_marketing_sup20citations['Cluster'])
silhouette = silhouette_score(embeddings_2D, doc_marketing_sup20citations['Cluster'])

print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)

# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = doc_marketing_sup20citations[['X', 'Y', 'dc_creator', 'year', 'Cluster']]

# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='2D plot with Dimensionality reduction with PCA and Clustering with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()

pio.write_html(fig, file="2D_embeddings_PCA.html")
```
