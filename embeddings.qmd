---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "An embeddings analysis"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Libraries and loading data

```{r}
#| label: load-packages
#| message: false

library(tidyverse)
library(word2vec)
library(quanteda)
library(text)
library(udpipe)
library(text2vec)
```

## Loading data

```{r}
#| label: load-data


list_articles <- read.csv2("nlp_full_data_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("entry_number" = 1)
list_references <- read.csv2("nlp_references_final_18-08-2023.csv", encoding = "UTF-8") %>%
  rename("citing_art" = 1)
colnames(list_articles) <- gsub("\\.+", "_", colnames(list_articles)) # <1>
colnames(list_articles) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_articles)) # <2>
colnames(list_references) <- gsub("\\.+", "_", colnames(list_references))
colnames(list_references) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_references))


data_embeddings <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE) %>%
  filter(marketing == 1) %>%
  mutate("year" = substr(prism_coverDate, 7, 10)) %>%
  mutate(keywords = str_replace_all(authkeywords, "\\|", "")) %>%
  mutate(keywords = str_squish(keywords)) %>%
  mutate("combined_text" = paste0(dc_title,". ", dc_description, ". ", keywords))

write.csv(data_embeddings,"data_for_embeddings.csv")
data_embeddings <- read.csv("data_for_embeddings.csv")
embeddings <- read.csv("embeddings_bge.csv")
```

## A glimpse of data

```{r}
#| label: glimpse-data

data_embeddings %>%
  head(5) %>%
  select(entry_number, dc_creator, combined_text, year)
```

## A first Word2Vec embeddings analysis (skip-gram)

```{r}
#| label: word2vec

set.seed(42)
model <- word2vec(x = tolower(data_embeddings$combined_text), type = "skip-gram", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(model)


```

```{r}
lookslike <- predict(model, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## A Word2Vec embeddings analysis (cbow)

```{r}
#| label: word2vec

set.seed(42)
modelcbow <- word2vec(x = tolower(data_embeddings$combined_text), type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
embedding1 <- as.matrix(modelcbow)
#embedding <- predict(modelcbow, c("mining"), type = "embedding")
#embedding
```

```{r}
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike

lookslike %>%
  ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
  geom_point()


lookslike %>%
  ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
  geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
  theme_minimal() +
  labs(x = "similarity score",
       y = "") +
  ggtitle("Top 20 similarity scores with term 'text'" ) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Part of speech tagging with UDPipe

```{r}
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
write.csv(annotated_text,"annotated_udpipe.csv")
```

## Top 20 nouns with UDPipe

```{r}

lemma <- annotated_text %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Part of speech tagging with Trankit (@nguyen2021trankit)

```{python}
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize

df = pd.read_csv("data_for_embeddings.csv")

# initialize a pipeline for English
p = Pipeline('english')

#test = p.posdep(df.loc[:5]['combined_text'][1])
#testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
  
#pos = p.posdep(all)

results = pd.DataFrame()  
#part of speech tagging
for text in df['combined_text']:
    pos = p.posdep(text)
    pos_df = pd.json_normalize(pos['sentences'], 'tokens')
    results = pd.concat([results,pos_df])

#lemmatization
results_lemma = pd.DataFrame()  
for text in df['combined_text']:
    lemma = p.lemmatize(text)
    lemma_df = pd.json_normalize(lemma['sentences'], 'tokens')
    results_lemma = pd.concat([results_lemma,lemma_df])
    
#join both data frames
results_complete = pd.concat([results, results_lemma['text'].rename("lemma")], axis=1)
results_complete["lemma"] = results_complete["text"]
results_lemma.to_csv("lemmas_trankit.csv")
results_complete.to_csv("annotated_trankit.csv")
```

## Top 20 nouns with Trankit

```{python}
import pandas as pd
import plotly.express as px

# Charger les données à partir du DataFrame "results"
# Assurez-vous que "results" contient les mêmes colonnes que le fichier CSV original
# (par exemple, "upos" et "lemma")
results_complete = pd.read_csv("annotated_trankit.csv")

# Filtrer les lignes où 'upos' est égal à "NOUN"
noun_data = results_complete[results_complete['upos'] == 'NOUN']

# Regrouper par 'lemma' et compter le nombre d'occurrences
top_nouns = noun_data['lemma'].value_counts().reset_index()
top_nouns.columns = ['lemma', 'n']
top_nouns = top_nouns.head(20)

# Créer un graphique à l'aide de Plotly Express
fig = px.scatter(top_nouns, x='n', y='lemma', color='lemma',
                 labels={'n': 'Frequency', 'lemma': 'Lemma'},
                 title='Top 20 Most Frequent Nouns')

# Personnaliser le style du graphique
fig.update_traces(marker=dict(size=12, opacity=0.6),
                  selector=dict(mode='markers'))

fig.update_layout(title_x=0.5, title_font=dict(size=20))
fig.update_layout(template="plotly_white")


# Afficher le graphique
fig.show()
fig.write_html("top_20_nouns_trankit_python.html")



```

## Part of speech tagging with Stanza (@qi2020stanza)

```{python}
import stanza
import pandas as pd
from tqdm import tqdm

# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)

# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")


annotated_df = pd.DataFrame()

for text in tqdm(df['combined_text'], desc="Processing Texts"):
    doc = nlp(text)
    dicts = doc.to_dict()
    
    # Convertissez le dictionnaire en un DataFrame temporaire
    temp_df = pd.DataFrame(dicts[0])
    
    # Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
    annotated_df = pd.concat([annotated_df, temp_df], ignore_index=True)



annotated_df.to_csv("annotated_stanza.csv")

```

## Top 20 nouns with Stanza

```{r}
annotation_stanza <- read.csv("annotated_stanza.csv")
lemma <- annotation_stanza %>%
  filter(upos == "NOUN") %>%
  group_by(lemma) %>%
  summarize(n = n()) %>%
  top_n(20)

ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
  geom_point(color = "royalblue") +
  theme_minimal() +
  labs(x = "Frequency",
       y = "Lemma") +
  ggtitle("Top 20 Most Frequent Nouns") +
  theme(plot.title = element_text(hjust = 0.5))
```

## **Custom Embeddings model for BERTopic** (all-MiniLM-L6-v2)

::: callout-note
We use a CountVectorizer which enables us to specify the range of the ngram we want to take an interest in. We can use it before or after the topic modelling (fine tuning).\
Here we use it before to exclude english stopwords, but after the embeddings process so that the foundation provided by stopwords in sentences is preserved in context.
:::

```{python}
#import warnings
#warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
from sklearn.feature_extraction.text import CountVectorizer

# create a countvectorizer object with specified settings:
# - ngram_range=(1, 3): generate both unigrams, bigrams and trigrams from the text.
#   this means it will consider individual words as well as pairs of consecutive words.
# - stop_words="english": use a predefined list of English stop words to exclude common words like "the," "and," etc.

vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words="english")

df = pd.read_csv("data_for_embeddings.csv")

docs_marketing = df["combined_text"].tolist()


# create a sentence model using the "all-MiniLM-L6-v2" pre-trained model
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

# encode the 'docs_marketing' text data into embeddings
# and show a progress bar while processing
embeddings = sentence_model.encode(docs_marketing, show_progress_bar=True)

# create an instance of BERToopic with vectorizer_model and other parameters
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)

# adjust BERTopic to embeddings. It extracts topics and corresponding probabilities.
topics, probs = topic_model.fit(docs_marketing, embeddings)
```

## **Custom Embeddings model for BERTopic** (bge-large-en)

```{python}

#model = SentenceTransformer('BAAI/bge-large-zh')
#embeddings_1 = model.encode(sentences, normalize_embeddings=True)
#embeddings_2 = model.encode(sentences, normalize_embeddings=True)
#similarity = embeddings_1 @ embeddings_2.T
#print(similarity)


sentence_model_bge = SentenceTransformer('BAAI/bge-large-en')
embeddings_bge = sentence_model_bge.encode(docs_marketing, show_progress_bar=True, normalize_embeddings=True)

# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model_bge = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)

# Ajustez le modèle BERTopic aux embeddings
topics_bge, probs_bge = topic_model_bge.fit_transform(docs_marketing, embeddings)
```

## Visualize how each token contributes to a specific topic. 

We can do it by selecting a sentence in the document. To do so, we need to first calculate topic distributions on a token level and then visualize the results:\
More parameters here :\
https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html

```{python}
# Calculate the topic distributions on a token-level, we can add the window
topic_distr, topic_token_distr = topic_model_bge.approximate_distribution(docs_marketing, calculate_tokens=True)

# Visualize the token-level distributions, put id of document
df_topicmodel = topic_model_bge.visualize_approximate_distribution(docs_marketing[150], topic_token_distr[150])
df_topicmodel

topics_html = df_topicmodel.to_html()

with open('topics_html.html', 'w') as html_file:
    html_file.write(topics_html)
```

## Get some information about topics and plot them

### What is the representative document for each topic?

```{python}
topicsinfo = topic_model_bge.get_topic_info()
topicsinfo
```

### Display representative words for each topic

```{python}
from tabulate import tabulate

data_as_list = topicsinfo.values.tolist()

# get coluumn names as headers
headers = topicsinfo.columns.tolist()

# print as a table
table = tabulate(data_as_list, headers, tablefmt='html')

with open('table_topics.html', 'w') as f:
    f.write(table)
```

```{python}

topic_model_bge.visualize_topics()
fig_topics_bge = topic_model_bge.visualize_topics()
fig_topics_bge.write_html("topics_bge_test.html")

fig_hierarchy_bge = topic_model_bge.visualize_hierarchy()
fig_hierarchy_bge.write_html("hierarchy_bge_test.html")

hierarchical_topics = topic_model_bge.hierarchical_topics(docs_marketing)
fig_hierarchical_topics_bge = topic_model_bge.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics_bge.write_html("hierarchical_topics_bge.html")

fig_barchart_bge = topic_model_bge.visualize_barchart()
fig_barchart_bge.write_html("barchart_bge_test.html")

fig_heatmap_bge = topic_model_bge.visualize_heatmap()
fig_heatmap_bge.write_html("fig_heatmap_bge.html")

```

## Save BERTopic model

```{python}
# Method 1 - safetensors
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 2 - pytorch
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="pytorch", save_ctfidf=True, save_embedding_model=embedding_model)

# Method 3 - pickle
topic_model.save("my_model", serialization="pickle")


#save my topic model
embedding_model = "BAAI/bge-large-zh"
topic_model.save("embeddings_bge_model",serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)


#save my embeddings model
embeddings_df = pd.DataFrame(embeddings)
embeddings_df.to_csv("embeddings_bge.csv", index=False)
```

## Project 3D embeddings (bge-large-en)

```{python}
import pandas as pd
import hdbscan
import plotly.express as px
from sklearn.decomposition import PCA
import plotly.io as pio
import seaborn as sns
import matplotlib.pyplot as plt
import time
import umap.umap_ as umap
from sklearn.metrics import davies_bouldin_score
from sklearn.metrics import silhouette_score, silhouette_samples
from yellowbrick.cluster import SilhouetteVisualizer

df = pd.read_csv("data_for_embeddings.csv")
embeddings_bge = pd.read_csv('embeddings_bge.csv')

##plot seaborn
plt.figure(figsize=(16,10))
fig = sns.scatterplot(
    x="X", y="Y",
    hue="Cluster",
    palette=sns.color_palette("hls", 10),
    data=df_vis,
    legend="full",
    alpha=0.3
)
fig = fig.get_figure()
fig.savefig("out.svg") 

```

## 3D plot of embeddings with PCA as dimensional reduction technique

```{python}

# Étape 1 : Réduction des dimensions avec PCA
pca = PCA(n_components=3)
embeddings_3D = pca.fit_transform(embeddings_bge)


# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_3D[:, 0]
df['Y'] = embeddings_3D[:, 1]
df['Z'] = embeddings_3D[:, 2]

print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))


# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_3D)

#evaluate clustering
davies_bouldin_score(embeddings_3D, df['Cluster'])
silhouette_score(embeddings_3D, df['Cluster'])

    
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'Z', 'dc_creator', 'year', 'Cluster']]

# Étape 5 : Création du graphique 3D interactif avec Plotly
fig = px.scatter_3d(df_vis, x='X', y='Y', z='Z', color='Cluster', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Clustering 3D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()

pio.write_html(fig, file="3D_embeddings_PCA.html")

```

## 2D plot of embeddings with TSNE as dimensional reduction technique

```{python}

time_start = time.time()
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(embeddings_bge)

print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))

df['tsne-2d-one'] = tsne_results[:,0]
df['tsne-2d-two'] = tsne_results[:,1]

davies_bouldin_score(tsne_results, labels)
silhouette_score(tsne_results, labels)

plt.figure(figsize=(16,10))
sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    hue="y",
    palette=sns.color_palette("hls", 10),
    data=df,
    legend="full",
    alpha=0.3
)

plt.savefig("tsne_2D.svg", format="svg")
```

## 3D plot of embeddings with UMAP as dimensional reduction technique

```{python}

# Étape 1 : Réduction des dimensions avec UMAP
umap_model = umap.UMAP(n_neighbors=15, n_components=3, min_dist=0.1, random_state=42)
umap_embeddings = umap_model.fit_transform(embeddings_bge)

plt.scatter(u[:,0], u[:,1], c=data)
plt.title('UMAP embedding of random colours');
plt.savefig("umap.svg", format="svg")

# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = umap_embeddings[:, 0]
df['Y'] = umap_embeddings[:, 1]
df['Z'] = umap_embeddings[:, 2]

print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))


# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_3D)

# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'Z', 'dc_creator', 'year', 'Cluster']]

# Étape 5 : Création du graphique 3D interactif avec Plotly
fig = px.scatter_3d(df_vis, x='X', y='Y', z='Z', color='Cluster', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Clustering 3D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()








embeddings = df[['embedding_1', 'embedding_2', 'embedding_3']]  # Remplacez par les noms de vos colonnes d'embeddings
umap_model = umap.UMAP(n_neighbors=15, n_components=3, min_dist=0.1, random_state=42)
umap_embeddings = umap_model.fit_transform(embeddings)
umap_df = pd.DataFrame(data=umap_embeddings, columns=['UMAP_1', 'UMAP_2', 'UMAP_3'])

```
