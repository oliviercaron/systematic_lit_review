df = pd.read_csv("data_for_embeddings.csv")
# Sélectionnez les 5 premières lignes de df['combined_text']
df_subset = df.head(5)
testdf = pd.DataFrame()
for text in tqdm(df_subset['combined_text'], desc="Processing Texts"):
doc = nlp(text)
dicts = doc.to_dict()
# Convertissez le dictionnaire en un DataFrame temporaire
temp_df = pd.DataFrame(dicts[0])
# Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
testdf = pd.concat([testdf, temp_df], ignore_index=True)
# À la fin de la boucle, testdf contiendra les données traitées pour les 5 lignes sélectionnées
import stanza
import pandas as pd
from tqdm import tqdm
# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)
# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")
# Sélectionnez les 5 premières lignes de df['combined_text']
df_subset = df.head(5)
testdf = pd.DataFrame()
for text in tqdm(df_subset['combined_text'], desc="Processing Texts"):
doc = nlp(text)
dicts = doc.to_dict()
# Convertissez le dictionnaire en un DataFrame temporaire
temp_df = pd.DataFrame(dicts[0])
# Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
testdf = pd.concat([testdf, temp_df], ignore_index=True)
# À la fin de la boucle, testdf contiendra les données traitées pour les 5 lignes sélectionnées
View(testdf)
import stanza
import pandas as pd
from tqdm import tqdm
# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)
# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")
annotated_df = pd.DataFrame()
for text in tqdm(df_subset['combined_text'], desc="Processing Texts"):
doc = nlp(text)
dicts = doc.to_dict()
# Convertissez le dictionnaire en un DataFrame temporaire
temp_df = pd.DataFrame(dicts[0])
# Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
annotated_df = pd.concat([annotated_df, temp_df], ignore_index=True)
annotated_df.to_csv("annotated_stanza.csv")
import stanza
import pandas as pd
from tqdm import tqdm
# Initialisation du modèle Stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', use_gpu=True, tokenize_pretokenized=False, tokenize_no_ssplit=True)
# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")
annotated_df = pd.DataFrame()
for text in tqdm(df['combined_text'], desc="Processing Texts"):
doc = nlp(text)
dicts = doc.to_dict()
# Convertissez le dictionnaire en un DataFrame temporaire
temp_df = pd.DataFrame(dicts[0])
# Ajoutez les données du DataFrame temporaire à testdf en ignorant l'index
annotated_df = pd.concat([annotated_df, temp_df], ignore_index=True)
annotated_df.to_csv("annotated_stanza.csv")
View(annotated_df)
quit
View(annotated_text)
View(data_embeddings)
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =40, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
write.csv(annotated_text,"annotated_udpipe.csv")
reticulate::repl_python()
View(annotated_df)
quit
annotation_stanza <- read.csv("annotated_stanza.csv")
lemma <- annotation_stanza %>%
filter(upos == "NOUN") %>%
group_by(lemma) %>%
summarize(n = n()) %>%
top_n(20)
ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue") +
theme_minimal() +
labs(x = "Frequency",
y = "Lemma") +
ggtitle("Top 20 Most Frequent Nouns") +
theme(plot.title = element_text(hjust = 0.5))
View(annotated_text)
View(list_articles)
View(embedding)
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
---
title: "Systematic literature review"
reticulate::repl_python()
import pandas as pd
from sentence_transformers import SentenceTransformer
# Prepare embeddings
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)
# Train our topic model using our pre-trained sentence-transformers embeddings
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
from sklearn.datasets import fetch_20newsgroups
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)
View(embeddings)
topic_model = BERTopic()
from bertopic import BERTopic
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
View(embeddings)
embedd_df = pd.dataFrame(embeddings)
embedd_df = pd.DataFrame(embeddings)
View(embedd_df)
topics, probs = topic_model.fit_transform(docs, embedd_df)
topics, probs = topic_model.fit_transform(docs, embeddings)
numpy
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
from bertopic import BERTopic
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
View(topic_model)
import pynndescent
pynn_dist_fns_fda = pynndescent.distances.fast_distance_alternatives
pynn_dist_fns_fda["cosine"]["correction"] = correct_alternative_cosine
pynn_dist_fns_fda = pynndescent.distances.fast_distance_alternatives
pynn_dist_fns_fda["cosine"]["correction"] = correct_alternative_cosine
pynn_dist_fns_fda["dot"]["correction"] = correct_alternative_cosine
topic_model = BERTopic(representation_model=representation_model)
from bertopic.representation import PartOfSpeech
topic_model = BERTopic(representation_model=representation_model)
representation_model = PartOfSpeech("en_core_web_sm")
representation_model = PartOfSpeech("en_core_web_sm")
View(docs)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
from umap import umap
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
import nuumpy as np
import numpy as np
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = model.fit_transform(embeddings)
model = BERTopic()
topics, probs = model.fit_transform(embeddings)
model = BERTopic()
# Ajustez le modèle aux embeddings
topics, probs = model.fit_transform(embeddings)
from bertopic import BERTopic
import numpy as np
model = BERTopic()
# Ajustez le modèle aux embeddings
topics, probs = model.fit_transform(embeddings)
# Sujets attribués à chaque document
print(topics)
# Probabilités d'appartenance aux sujets
print(probs)
topics, probs = model.fit_transform(embedd_df)
topics, probs = model(embeddings)
from bertopic import BERTopic
import numpy as np
model = BERTopic()
# Ajustez le modèle aux embeddings
topics, probs = model(embeddings)
# Sujets attribués à chaque document
print(topics)
# Probabilités d'appartenance aux sujets
print(probs)
print(topics)
model = BERTopic()
topics, probs = model(embeddings)
topics, probs = model.fit_transform(embeddings)
embeddings_list = embeddings.tolist()
()
topics, probs = model.fit_transform(embeddings_list)
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embedding_list)
topics, probs = topic_model.fit_transform(docs, embeddings_list)
topics, probs = topic_model.fit_transform(docs, embeddings)
View(embeddings_list)
topic_model = BERTopic()
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
View(embeddings)
topics, probs = topic_model.fit_transform(docs, embeddings)
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
from sklearn.feature_extraction.text import CountVectorizer
nt
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
verbose=True)
model = BERTopic(vectorizer_model=vectorizer_model,
language='english', calculate_probabilities=True,
verbose=True)
topics, probs = model.fit_transform(embeddings_list)
model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
topics, probs = model.fit_transform(embeddings_list)
topic_model = BERTopic(model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(docs)
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model.get_topic_info()
topic_model.get_topic(0)
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs[:5], show_progress_bar=True)
# Train our topic model using our pre-trained sentence-transformers embeddings
topic_model = BERTopic(model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.datasets import fetch_20newsgroups
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
# Prepare embeddings
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs[:5], show_progress_bar=True)
# Create a BERTopic instance with your vectorizer_model and other settings
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Fit the topic model using the embeddings
topics, probs = topic_model.fit_transform(embeddings)
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"]
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Train our topic model using our pre-trained sentence-transformers embeddings
topic_model = BERTopic(model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(doc, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5].list
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
docs = df["combined_text"][:5].tolist()
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5].tolist()
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
from sentence_transformers import SentenceTransformer
# Prepare embeddings
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)
# Train our topic model using our pre-trained sentence-transformers embeddings
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
from sentence_transformers import SentenceTransformer
# Prepare embeddings
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)
# Train our topic model using our pre-trained sentence-transformers embeddings
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2))
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True, n_topics=10)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA  # Ajout de l'importation de PCA
# Chargement du DataFrame depuis le fichier CSV
df = pd.read_csv("data_for_embeddings.csv")
# Préparation des embeddings
docs = df["combined_text"][:5]
# Création du modèle de phrases
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Réduction de dimensionnalité avec PCA
pca = PCA(n_components=100)  # Réduction à 100 dimensions (ajustez selon vos besoins)
reduced_embeddings = pca.fit_transform(embeddings)
# Création d'une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustement du modèle BERTopic aux embeddings réduits
topics, probs = topic_model.fit_transform(reduced_embeddings)
from sklearn.decomposition import PCA  # Ajout de l'importation de PCA
docs = df["combined_text"][:5]
# Création du modèle de phrases
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Réduction de dimensionnalité avec PCA
pca = PCA(n_components=100)  # Réduction à 100 dimensions (ajustez selon vos besoins)
reduced_embeddings = pca.fit_transform(embeddings)
pca = PCA(n_components=5)  # Réduction à 100 dimensions (ajustez selon vos besoins)
reduced_embeddings = pca.fit_transform(embeddings)
# Création d'une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustement du modèle BERTopic aux embeddings réduits
topics, probs = topic_model.fit_transform(reduced_embeddings)
topics, probs = topic_model.fit_transform(docs, reduced_embeddings)
View(pca)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2))
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True, n_topics=10)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2))
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5]
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
topics, probs = topic_model.fit_transform(embeddings)
docs = df["combined_text"][:5].to_string()
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import numpy as np
import pandas as pd  # Ajout de l'importation de pandas
vectorizer_model = CountVectorizer(ngram_range=(1, 2))
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5].to_string()
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
vectorizer_model = CountVectorizer(ngram_range=(1, 2))
# Prepare embeddings
# Assurez-vous d'avoir importé et chargé votre DataFrame 'df'
df = pd.read_csv("data_for_embeddings.csv")
docs = df["combined_text"][:5].to_string()
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Créez une instance de BERTopic avec votre vectorizer_model et d'autres paramètres
topic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True, verbose=True)
# Ajustez le modèle BERTopic aux embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
View(docs)
docs
docs[0]
docs[1]
docs[3]
View(docs)
docs[[3]]
topics, probs = topic_model.fit_transform(docs, embeddings)
