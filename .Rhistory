calculate_probabilities=True,
verbose=True,
min_topic_size=min_topic_size,
nr_topics=nr_topics
)
# fit the bertopic model to the input documents and embeddings
topics, probs = topic_model.fit_transform(docs, embeddings)
# update the vectorizer model used by bertopic
# `min_df` is the minimum document frequency for terms (words or n-grams) in the CountVectorizer.
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3), min_df=3)
topic_model.update_topics(docs, vectorizer_model=updated_vectorizer_model)
# return the trained bertopic model
return topic_model
#topic_model_test = create_bertopic(docs_marketing, "allenai-specter", 5, 13)
#model_name = "test"
#visualize_bertopic(topic_model_test, model_name)
#| label: viz-bertopic
def generate_topics_table(topic_model):
# get topic information from the model
topics_info = topic_model.get_topic_info()
#  check if topics_info is empty or None
if topics_info is None or topics_info.empty:
return "No topics found."
# convert the data into a list
data_as_list = topics_info.values.tolist()
# get column names as headers
headers = topics_info.columns.tolist()
# generate the table in HTML format
table = tabulate(data_as_list, headers, tablefmt='html')
return table
def visualize_bertopic(topic_model, model_name, nr_topics):
# create the "images" folder if it doesn't exist already
if not os.path.exists("images"):
os.makedirs("images")
# create a subfolder for the specific topic model
model_folder = os.path.join("images", model_name+"-"+str(nr_topics)+"topics")
# create the model folder if it doesn't exist already
if not os.path.exists(model_folder):
os.makedirs(model_folder)
else:
# delete existing files in the model folder if it exists
for file in os.listdir(model_folder):
os.remove(os.path.join(model_folder, file))
# generate topics information table
topics_table = generate_topics_table(topic_model)
with open(os.path.join(model_folder, 'table_topics.html'), 'w') as f:
f.write(topics_table)
# visualize topics
fig_topics = topic_model.visualize_topics()
fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))
# visualize hierarchy
fig_hierarchy = topic_model.visualize_hierarchy()
fig_hierarchy.write_html(os.path.join(model_folder, "hierarchy.html"))
# visualize hierarchical topics
hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
fig_hierarchical_topics = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics.write_html(os.path.join(model_folder, "hierarchical.html"))
# visualize the bar chart
fig_barchart = topic_model.visualize_barchart(width=300, height=300, n_words=10, topics=None, top_n_topics=20)
fig_barchart.write_html(os.path.join(model_folder, "barchart.html"))
# visualize the heatmap
fig_heatmap = topic_model.visualize_heatmap()
fig_heatmap.write_html(os.path.join(model_folder, "heatmap.html"))
# topics over time
years =  df['year'].to_list()
topics_over_time = topic_model.topics_over_time(docs_marketing, years)
fig_topics_over_time = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, normalize_frequency=True)
fig_topics_over_time.write_html(os.path.join(model_folder, "topicsovertime.html"))
#| label: viz-bertopic
def generate_topics_table(topic_model):
# get topic information from the model
topics_info = topic_model.get_topic_info()
#  check if topics_info is empty or None
if topics_info is None or topics_info.empty:
return "No topics found."
# convert the data into a list
data_as_list = topics_info.values.tolist()
# get column names as headers
headers = topics_info.columns.tolist()
# generate the table in HTML format
table = tabulate(data_as_list, headers, tablefmt='html')
return table
def visualize_bertopic(topic_model, model_name, nr_topics):
# create the "images" folder if it doesn't exist already
if not os.path.exists("images"):
os.makedirs("images")
# create a subfolder for the specific topic model
model_folder = os.path.join("images", model_name+"-"+str(nr_topics)+"topics")
# create the model folder if it doesn't exist already
if not os.path.exists(model_folder):
os.makedirs(model_folder)
else:
# delete existing files in the model folder if it exists
for file in os.listdir(model_folder):
os.remove(os.path.join(model_folder, file))
# generate topics information table
topics_table = generate_topics_table(topic_model)
with open(os.path.join(model_folder, 'table_topics.html'), 'w') as f:
f.write(topics_table)
# visualize topics
fig_topics = topic_model.visualize_topics()
fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))
# visualize hierarchy
fig_hierarchy = topic_model.visualize_hierarchy()
fig_hierarchy.write_html(os.path.join(model_folder, "hierarchy.html"))
# visualize hierarchical topics
hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
fig_hierarchical_topics = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics.write_html(os.path.join(model_folder, "hierarchical.html"))
# visualize the bar chart
fig_barchart = topic_model.visualize_barchart(width=300, height=300, n_words=10, topics=None, top_n_topics=20)
fig_barchart.write_html(os.path.join(model_folder, "barchart.html"))
# visualize the heatmap
fig_heatmap = topic_model.visualize_heatmap()
fig_heatmap.write_html(os.path.join(model_folder, "heatmap.html"))
# topics over time
years =  df['year'].to_list()
topics_over_time = topic_model.topics_over_time(docs_marketing, years)
fig_topics_over_time = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, normalize_frequency=True)
fig_topics_over_time.write_html(os.path.join(model_folder, "topicsovertime.html"))
#| label: create-bertopics
#09/26/2023 -----------------------------------------
#to-do  clean code : embeddings are charged twice for viz purposes (document viz) because it is loaded a first time in the create_bertopic function and again in the loop (can't use just the model name in visualize_bertopic function.)
#---------------------------------------------------
# list of embeddings models
list_embeddings = ["all-mpnet-base-v2", "all-mpnet-base-v1"]
#list_embeddings = ["all-mpnet-base-v2","multi-qa-mpnet-base-dot-v1","all-roberta-large-v1","all-MiniLM-L12-v2"]
# create a list to store model information
table_data = []
topic_models = {}
#nbtopics is the number of topics we want to create/reduce to
#nbmintopicsize is the minimum number of documents to form a topic
nbtopics = 17
nbmintopicsize = 5
# loop through the list of embeddings models and create topic_model + viz in images
for embeddings_model in list_embeddings:
print(f"\nCreating BERTopics with the {embeddings_model} Sentence-Transformers pretrained model.")
topic_model = create_bertopic(docs_marketing, embeddings_model, nbmintopicsize, nbtopics)
print(f"\nCreating BERTopic visualizations in the `images\\{embeddings_model}-{nbtopics}topics` folder.")
visualize_bertopic(topic_model, embeddings_model, nbtopics)
chargedmodel = SentenceTransformer(embeddings_model, device='cuda')
# visualize the documents
model_folder = os.path.join("images", embeddings_model+"-"+str(nbtopics)+"topics")
embeddings = chargedmodel.encode(docs_marketing, show_progress_bar=False)
fig_documents = topic_model.visualize_documents(docs_marketing, embeddings=embeddings)
fig_documents.write_html(os.path.join(model_folder, "documents_topics.html"))
# to summarize embeddings' models
dimensions =  chargedmodel.get_sentence_embedding_dimension()
max_tokens = chargedmodel.max_seq_length
# store the topic_model in the dictionary with the embeddings name as key
topic_models[embeddings_model] = topic_model
# add model information to the table data list
table_data.append([embeddings_model, dimensions, max_tokens])
# table headers
headers = ["Embeddings Model", "Dimensions", "Max Tokens"]
# title for the table, centered
table_title = "Summary of Embeddings Models used"
# create the table with centered title
table = tabulate(table_data, headers, tablefmt="pretty")
table_lines = table.split("\n")
table_lines.insert(0, table_title.center(len(table_lines[0])))
table_with_centered_title = "\n".join(table_lines)
# display the table with centered title
print("\n")
print(table_with_centered_title)
#sentence_model.max_seq_length
#sentence_model.get_sentence_embedding_dimension()
quit
embeddings_model_name = "all-mpnet-base-v2"
reticulate::repl_python()
embeddings_model_name = "all-mpnet-base-v2"
topics_list = topic_models[embeddings_model_name]["model"].get_topics()
topics_list = topic_models[embeddings_model_name]["model"].get_topics()
topics_list = topic_models[embeddings_model_name]["model"].get_topics()
embeddings_model_name = "all-mpnet-base-v2"
topics_list = topic_models[embeddings_model_name]["model"].get_topics()
topics_list = topic_models[embeddings_model_name]["model"]
View(topic_models)
View(topic_models)
topic_models[embeddings_model_name]
print(topic_models[embeddings_model_name])
print(topic_models[embeddings_model_name]["model"])
View(topic_models)
print(topic_models[embeddings_model_name].topics_)
len(topic_models[embeddings_model_name].topics_)
topic_models[embeddings_model_name].calculate_probabilities
topic_models[embeddings_model_name]._extract_words_per_topic
topic_models[embeddings_model_name].probabilities_
len(topic_models[embeddings_model_name].probabilities_)
len(topic_models[embeddings_model_name].topics_)
len(topic_models[embeddings_model_name].probabilities_)
len(topic_models[embeddings_model_name].topics_)
type(topic_models[embeddings_model_name].topics_)
topics_list = topic_models[embeddings_model_name].topics_
df["topic"] = topics_list
View(df)
topic_models[embeddings_model_name].get_topic_info
print(topic_models[embeddings_model_name].get_topic_info)
print(topic_models[embeddings_model_name].get_topic_info())
topic_models[embeddings_model_name].get_topic_info()
embeddings_model_name = "all-mpnet-base-v2"
bertopic_model = topic_models[embeddings_model_name]["model"]
topic_infp_df = topic_models[embeddings_model_name].get_topic_info()
topic_info_df = topic_models[embeddings_model_name].get_topic_info()
topic_info_df
view(topic_info_df)
selected_columns = topic_info_df[["Topic", "Name"]]
selected_columns
# we can access the different topic models like this in the topics_models dictionnary
embeddings_model_name = "all-mpnet-base-v2"
topics_list = topic_models[embeddings_model_name]["model"]
#len(topic_models[embeddings_model_name].probabilities_) # = 405 like the docs_marketings
#len(topic_models[embeddings_model_name].topics_)        # = 405 like the docs_marketings
#type(topic_models[embeddings_model_name].topics_)      # list
#put the topics' number in the df of marketing documents
df["topic"] = topics_list
topic_info_df = topic_models[embeddings_model_name].get_topic_info()
selected_columns = topic_info_df[["Topic", "Name"]]
# Continue from your existing code
df["topic_name"] = df["topic"].map(selected_columns.set_index("Topic")["Name"])
topics_list = topic_models[embeddings_model_name].topics_
# we can access the different topic models like this
embeddings_model_name = "all-mpnet-base-v2"
topics_list = topic_models[embeddings_model_name].topics_
#len(topic_models[embeddings_model_name].probabilities_) # = 405 like the docs_marketings
#len(topic_models[embeddings_model_name].topics_)        # = 405 like the docs_marketings
#type(topic_models[embeddings_model_name].topics_)      # list
#put the topics' number in the df of marketing documents
df["topic"] = topics_list
topic_info_df = topic_models[embeddings_model_name].get_topic_info()
selected_columns = topic_info_df[["Topic", "Name"]]
df["topic_name"] =
df["topic"] = topics_list
topic_info_df = topic_models[embeddings_model_name].get_topic_info()
topic_info_df
selected_columns = topic_info_df[["Topic", "Name"]]
df["topic_name"] = df["topic"].map(selected_columns.set_index("Topic")["Name"])
View(df)
topic_counts = df["topic"].value_counts()
topic_counts
topic_counts
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=500, random_state=42, verbose=1)
#| label: load-data-packages-python
import warnings
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")
from tqdm import tqdm
from transformers import XLNetTokenizer, XLNetModel
from sentence_transformers import SentenceTransformer, util
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples
from yellowbrick.cluster import SilhouetteVisualizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from tabulate import tabulate
import nltk
import numpy as np
import pandas as pd
import hdbscan
import plotly.express as px
import plotly.io as pio
import seaborn as sns
import matplotlib.pyplot as plt
import time
import umap.umap_ as umap
import torch
import os
import string
df = pd.read_csv("data_for_embeddings.csv")
df['title_abstract'] = df['dc_title'].astype(str) + '. ' + df['dc_description'].astype(str)
docs_marketing = df["combined_text"].tolist()
#| label: extract-topics-bertopic
# we can access the different topic models like this
embeddings_model_name = "all-mpnet-base-v2"
topics_list = topic_models[embeddings_model_name].topics_
# len(topic_models[embeddings_model_name].probabilities_) # = 405 like the docs_marketings
# len(topic_models[embeddings_model_name].topics_)        # = 405 like the docs_marketings
# type(topic_models[embeddings_model_name].topics_)      # list
# put the topics' number in the df of marketing documents
df["topic"] = topics_list
# get the correspondance between topic number and topic name
topic_info_df = topic_models[embeddings_model_name].get_topic_info()
selected_columns = topic_info_df[["Topic", "Name"]]
# then put it into df
df["topic_name"] = df["topic"].map(selected_columns.set_index("Topic")["Name"])
# for info, the distribution of topics (many outliers)
topic_counts = df["topic"].value_counts()
topic_counts
#| label: plot-
# Perform t-SNE to reduce the word vectors to 3D
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=500, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=1000, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=5000, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=150, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=250, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=800, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=600, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=800, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=1000, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=1337, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
tsne_result = tsne.fit_transform(embeddings)
tsne_result[:, 2]
tsne_result[:, 3]
df['X'] = tsne_result[:, 0]
df['Y'] = tsne_result[:, 1]
df['Z'] = tsne_result[:, 2]
davies_bouldin = davies_bouldin_score(tsne_result, df['topic'])
silhouette = silhouette_score(tsne_result, df['topic'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
df_vis = df[['X', 'Y', 'Z', 'dc_creator', 'year', 'topic']]
fig = px.scatter_3d(df_vis, x='X', y='Y', z='Z', color='topic', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE and Clustering with BERTopic')
fig.update_layout(template="plotly_white")
fig.show()
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1337, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
davies_bouldin = davies_bouldin_score(tsne_result, df['topic'])
silhouette = silhouette_score(tsne_result, df['topic'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'topic']]
fig = px.scatter(df_vis, x='X', y='Y', color='topic', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1))
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE (2D) and Clustering with BERTopic')
fig.update_layout(template="plotly_white")
fig.show()
color_map = {
topic_name: f'rgb({r},{g},{b})' for topic_name, r, g, b in zip(df_vis['topic_name'].unique(), range(0, 256, 30), range(0, 256, 30), range(0, 256, 30))
}
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'topic_name']]  # Use 'topic_name' instead of 'topic'
# Define a color map for topic names
color_map = {
topic_name: f'rgb({r},{g},{b})' for topic_name, r, g, b in zip(df_vis['topic_name'].unique(), range(0, 256, 30), range(0, 256, 30), range(0, 256, 30))
}
fig = px.scatter(df_vis, x='X', y='Y', color='topic_name', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1),
color_discrete_map=color_map)  # Utilisez color_discrete_map pour sp√©cifier les couleurs
fig.update_traces(marker=dict(size=5))
fig.update_layout(template="plotly_white")
fig.update_layout(legend_title_text='Topic Names')
fig.show()
#| label: plot-authors-year-2D
# Perform t-SNE to reduce the word vectors to 2D
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1337, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
# Cluster evaluation
davies_bouldin = davies_bouldin_score(tsne_result, df['topic'])
silhouette = silhouette_score(tsne_result, df['topic'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Step 4: Create a DataFrame for 2D visualization
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'topic_name']]  # Use 'topic_name' instead of 'topic'
# Define a color map for topic names
color_map = {
topic_name: f'rgb({r},{g},{b})' for topic_name, r, g, b in zip(df_vis['topic_name'].unique(), range(0, 256, 30), range(0, 256, 30), range(0, 256, 30))
}
# Step 5: Create an interactive 2D plot with Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='topic_name', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1),
color_discrete_map=color_map)  # Use color_discrete_map to specify colors
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE (2D) and Clustering with BERTopic')
fig.update_layout(template="plotly_white")
# Add a custom legend title
fig.update_layout(legend_title_text='Topic Names')
fig.show()
pio.write_html(fig, file="2D_authors_embeddings_TSNE_BERTopic.html")
#| label: plot-authors-year-2D
# Perform t-SNE to reduce the word vectors to 2D
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1337, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
# Cluster evaluation
davies_bouldin = davies_bouldin_score(tsne_result, df['topic'])
silhouette = silhouette_score(tsne_result, df['topic'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Step 4: Create a DataFrame for 2D visualization
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'topic_name']]  # Use 'topic_name' instead of 'topic'
# Define a color map for topic names
color_map = {
topic_name: f'rgb({r},{g},{b})' for topic_name, r, g, b in zip(df_vis['topic_name'].unique(), range(0, 256, 30), range(0, 256, 30), range(0, 256, 30))
}
# Step 5: Create an interactive 2D plot with Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='topic_name', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1),
color_discrete_map=color_map, category_orders={"color": df_vis['topic_name'].unique()})  # Specify category_orders
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE (2D) and Clustering with BERTopic')
fig.update_layout(template="plotly_white")
# Add a custom legend title
fig.update_layout(legend_title_text='Topic Names')
fig.show()
pio.write_html(fig, file="2D_authors_embeddings_TSNE_BERTopic.html")
#| label: plot-authors-year-2D
# Perform t-SNE to reduce the word vectors to 2D
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1337, random_state=42, verbose=1)
tsne_result = tsne.fit_transform(embeddings)
# Cluster evaluation
davies_bouldin = davies_bouldin_score(tsne_result, df['topic'])
silhouette = silhouette_score(tsne_result, df['topic'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Step 4: Create a DataFrame for 2D visualization
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'topic_name']]  # Use 'topic_name' instead of 'topic'
# Define a color map for topic names
color_map = {
topic_name: f'rgb({r},{g},{b})' for topic_name, r, g, b in zip(df_vis['topic_name'].unique(), range(0, 256, 30), range(0, 256, 30), range(0, 256, 30))
}
# Step 5: Create an interactive 2D plot with Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='topic_name', text=df_vis.apply(lambda row: f"{row['dc_creator']}, {row['year']}", axis=1),
color_discrete_map=color_map)  # Use color_discrete_map to specify colors
fig.update_traces(marker=dict(size=5))
fig.update_layout(title='Dimensionality reduction with t-SNE (2D) and Clustering with BERTopic')
fig.update_layout(template="plotly_white")
# Add a custom legend title
fig.update_layout(legend_title_text='Topic Names')
fig.show()
pio.write_html(fig, file="2D_authors_embeddings_TSNE_BERTopic.html")
View(topic_models)
topics_list
topic_info_df
#| label: cuda
print(f"Is CUDA supported by this system? {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
# Storing ID of the current CUDA device
cuda_id = torch.cuda.current_device()
print(f"ID of the current CUDA device: {cuda_id}")
print(f"Name of the current CUDA device: {torch.cuda.get_device_name(cuda_id)}")
topic_model.dmm_results()
View(topic_models)
topic_models[embeddings_model_name].topics_
topic_models[embeddings_model_name].topics_[1]
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, , calculate_tokens=True)
topic_distr, topic_token_distr = topic_model.approximate_distribution(docs_marketing, calculate_tokens=True)
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
View(topic_token_distr)
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[150], topic_token_distr[150])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('topics_html.html', 'w') as html_file:
html_file.write(topics_html)
with open('topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
View(df)
#| label: token-contribution-topic
# Calculate the topic distributions on a token-level for the all-mpnet-base-v2 model, we can add the window
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
# Visualize the token-level distributions, put id of document
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[383], topic_token_distr[383])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
#| label: token-contribution-topic
# Calculate the topic distributions on a token-level for the all-mpnet-base-v2 model, we can add the window
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
# Visualize the token-level distributions, put id of document
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[449], topic_token_distr[449])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
#| label: token-contribution-topic
# Calculate the topic distributions on a token-level for the all-mpnet-base-v2 model, we can add the window
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
# Visualize the token-level distributions, put id of document
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[384], topic_token_distr[384])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
#| label: token-contribution-topic
# Calculate the topic distributions on a token-level for the all-mpnet-base-v2 model, we can add the window
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
# Visualize the token-level distributions, put id of document
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[382], topic_token_distr[382])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
#| label: token-contribution-topic
# Calculate the topic distributions on a token-level for the all-mpnet-base-v2 model, we can add the window
topic_distr, topic_token_distr = topic_models["all-mpnet-base-v2"].approximate_distribution(docs_marketing, calculate_tokens=True)
# Visualize the token-level distributions, put id of document
df_topicmodel = topic_models["all-mpnet-base-v2"].visualize_approximate_distribution(docs_marketing[382], topic_token_distr[382])
df_topicmodel
topics_html = df_topicmodel.to_html()
topics_html
with open('images/topics_contribution.html', 'w') as html_file:
html_file.write(topics_html)
from wordcloud import WordCloud
quit
title-block-banner: true
title-block-banner: true
