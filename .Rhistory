df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'Cluster']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=1))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'Cluster']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='black'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
df = pd.read_csv("data_for_embeddings.csv")
embeddings_bge = pd.read_csv('embeddings_bge.csv')
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
View(df)
df = pd.read_csv("data_for_embeddings.csv")
df = df[df['cited_by_count'] > 10]
embeddings_bge = pd.read_csv('embeddings_bge.csv')
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
df = pd.read_csv("data_for_embeddings.csv")
df = df[df['cited_by_count'] > 10]
df = df[df['citedby_count'] > 10]
df = pd.read_csv("data_for_embeddings.csv")
df = df[df['citedby_count'] > 10]
embeddings_bge = pd.read_csv('embeddings_bge.csv')
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
View(df)
df = df[df['citedby_count'] > 20]
df = df[df['citedby_count'] > 20]
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'Cluster']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
embeddings_2D = pca.transform(embeddings_bge)
df = df[df['citedby_count'] > 20]
pca = PCA(n_components=2)
embeddings_2D = pca.transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'Cluster']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='Dimensionality reduction with PCA and Clustering 2D with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
df = df[df['citedby_count'] > 20]
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
# Étape 2 : Coller les embeddings réduits au DataFrame df
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))
# Étape 3 : Clustering avec HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=8, metric='euclidean')
df['Cluster'] = clusterer.fit_predict(embeddings_2D)
# Évaluation du clustering
davies_bouldin = davies_bouldin_score(embeddings_2D, df['Cluster'])
silhouette = silhouette_score(embeddings_2D, df['Cluster'])
print('Davies-Bouldin Score:', davies_bouldin)
print('Silhouette Score:', silhouette)
# Étape 4 : Création d'un DataFrame pour la visualisation
df_vis = df[['X', 'Y', 'dc_creator', 'year', 'Cluster']]
# Étape 5 : Création du graphique 2D interactif avec Plotly
fig = px.scatter(df_vis, x='X', y='Y', color='Cluster'))
fig.update_traces(marker=dict(size=7))
fig.update_layout(title='2D plot with Dimensionality reduction with PCA and Clustering with HDBSCAN')
fig.update_layout(template="plotly_white")
fig.show()
pio.write_html(fig, file="2D_embeddings_PCA.html")
del df, pca, embeddings_2D, fig, df_vis
df = pd.read_csv("data_for_embeddings.csv")
embeddings_bge = pd.read_csv('embeddings_bge.csv')
df = df[df['citedby_count'] > 20]
pca = PCA(n_components=2)
embeddings_2D = pca.fit_transform(embeddings_bge)
df['X'] = embeddings_2D[:, 0]
df['X'] = embeddings_2D[:, 0]
embeddings_2D = pca.fit_transform(embeddings_bge)
df['X'] = embeddings_2D[:, 0]
df['X'] = embeddings_2D[:, 0]
df['Y'] = embeddings_2D[:, 1]
df = df[df['citedby_count'] > 20]
doc_marketing_sup20citations = df['combined_text']
sentence_model_bge = SentenceTransformer('BAAI/bge-large-en')
sentence_model_bge = SentenceTransformer('BAAI/bge-large-en',device='cuda')
embeddings_bge_sup20citations = sentence_model_bge.encode(doc_marketing_sup20citations, show_progress_bar=True, normalize_embeddings=True)
print("Torch version:",torch.__version__)
import torch
print("Torch version:",torch.__version__)
print("Is CUDA enabled?",torch.cuda.is_available())
print("Is CUDA enabled?",torch.cuda.is_available())
print(f"Is CUDA supported by this system?
{torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
# Storing ID of current CUDA device
cuda_id = torch.cuda.current_device()
print(f"ID of current CUDA device:
{torch.cuda.current_device()}")
print(f"Name of current CUDA device:
{torch.cuda.get_device_name(cuda_id)}")
print(f"Is CUDA supported by this system?
{torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
# Storing ID of current CUDA device
cuda_id = torch.cuda.current_device()
print(f"ID of current CUDA device:
{torch.cuda.current_device()}")
print(f"Name of current CUDA device:
reticulate::repl_python()
import warnings
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")
from tqdm import tqdm
from transformers import XLNetTokenizer, XLNetModel
from sentence_transformers import SentenceTransformer, util
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples
from yellowbrick.cluster import SilhouetteVisualizer
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
import nltk
import numpy as np
import pandas as pd
import hdbscan
import plotly.express as px
import plotly.io as pio
import seaborn as sns
import matplotlib.pyplot as plt
import time
import umap.umap_ as umap
import torch
df = pd.read_csv("data_for_embeddings.csv")
df['title_abstract'] = df['dc_title'].astype(str) + '. ' + df['dc_description'].astype(str)
docs_marketing = df["combined_text"].tolist()
print(f"Is CUDA supported by this system? {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
# Storing ID of the current CUDA device
cuda_id = torch.cuda.current_device()
print(f"ID of the current CUDA device: {cuda_id}")
print(f"Name of the current CUDA device: {torch.cuda.get_device_name(cuda_id)}")
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
sentence_model = SentenceTransformer("all-mpnet-base-v2", device='cuda')
embeddings_mpnet = sentence_model.encode(docs_marketing, show_progress_bar=True, output_value='token_embeddings')
View(embeddings_mpnet)
embeddings_mpnet = sentence_model.encode(docs_marketing, show_progress_bar=True,convert_to_tensor=True)
View(embeddings_mpnet)
topic_model_mpnet = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
topics_mpnet, probs_mpnet = topic_model_mpnet.fit_transform(docs_marketing, embeddings_mpnet)
sentence_model = SentenceTransformer("all-mpnet-base-v2", device='cuda')
embeddings_mpnet = sentence_model.encode(docs_marketing, show_progress_bar=True)
topic_model_mpnet = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
topics_mpnet, probs_mpnet = topic_model_mpnet.fit_transform(docs_marketing, embeddings_mpnet)
pip install --upgrade numba
pip install numba -- update
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_mpnet.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
for sentence, embedding in zip(docs_marketing, embeddings_mpnet):
print("Sentence:", sentence)
print("Embedding:", embedding)
print("")
len(embeddings_mpnet)
len(embeddings_mpnet)[0][0]
len(embeddings_mpnet)[0]
len(embeddings_mpnet[0])
len(embeddings_mpnet[1])
topic_model_mpnet.visualize_documents
topic_model_mpnet.visualize_documents()
topic_model_mpnet.visualize_documents(doc_marketing)
topic_model_mpnet.visualize_documents()
import os
from tabulate import tabulate
def generate_topics_table(topic_model):
# get topic information from the model
topics_info = topic_model.get_topic_info()
# convert the data into a list
data_as_list = topics_info.values.tolist()
# get column names as headers
headers = topics_info.columns.tolist()
# generate the table in HTML format
table = tabulate(data_as_list, headers, tablefmt='html')
# save the table to an HTML file
with open('table_topics.html', 'w') as f:
f.write(table)
def visualize_bertopic(topic_model, model_name):
# create the "images" folder if it doesn't exist already
if not os.path.exists("images"):
os.makedirs("images")
# create a subfolder for the specific topic model
model_folder = os.path.join("images", model_name)
# Create the model folder if it doesn't exist
if not os.path.exists(model_folder):
os.makedirs(model_folder)
else:
# Delete existing files in the model folder
for file in os.listdir(model_folder):
os.remove(os.path.join(model_folder, file))
# generate topics information table
generate_topics_table(topic_model)
os.rename('table_topics.html', os.path.join(model_folder, 'table_topics.html'))
# visualize topics
fig_topics = topic_model.visualize_topics()
fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))
# visualize hierarchy
fig_hierarchy = topic_model.visualize_hierarchy()
fig_hierarchy.write_html(os.path.join(model_folder, "hierarchy.html"))
# visualize hierarchical topics
hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
fig_hierarchical_topics = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics.write_html(os.path.join(model_folder, "hierarchical.html"))
# visualize the bar chart
fig_barchart = topic_model.visualize_barchart(width=300, height=300, n_words=10, topics=None, top_n_topics=20)
fig_barchart.write_html(os.path.join(model_folder, "barchart.html"))
# visualize the heatmap
fig_heatmap = topic_model.visualize_heatmap()
fig_heatmap.write_html(os.path.join(model_folder, "heatmap.html"))
model_name = "all-mpnet-base-v2"
visualize_bertopic(topic_model_mpnet, model_name)
#for some reason, it does not work with stopwords in the CountVectorizer from BERTopic
years =  df['year'].to_list()
topics_over_time = topic_model_mpnet.topics_over_time(docs_marketing, years)
fig_topics_over_time = topic_model_mpnet.visualize_topics_over_time(topics_over_time, top_n_topics=20, normalize_frequency=True)
fig_topics_over_time.write_html("topics_over_time_mpnet_tfidfreduced.html")
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
# create a sentence model using the "all-mpnet-base-v2" pre-trained model
sentence_model = SentenceTransformer("all-roberta-large-v1", device='cuda')
# encode the 'docs_marketing' text data into embeddings
# and show a progress bar while processing
embeddings_roberta = sentence_model.encode(docs_marketing, show_progress_bar=True)
# create an instance of BERToopic with vectorizer_model and other parameters
topic_model_roberta = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
# adjust BERTopic to embeddings. It extracts topics and corresponding probabilities.
topics_roberta, probs_roberta = topic_model_mpnet.fit_transform(docs_marketing, embeddings_roberta)
#update topic
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_roberta.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_roberta.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
topic_model_roberta = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
topics_roberta, probs_roberta = topic_model_mpnet.fit_transform(docs_marketing, embeddings_roberta)
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_roberta.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
model_name = "roberta"
visualize_bertopic(topic_model_roberta, model_name)
topic_model_roberta = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
topics_roberta, probs_roberta = topic_model_mpnet.fit_transform(docs_marketing, embeddings_roberta)
topics_roberta, probs_roberta = topic_model_roberta.fit_transform(docs_marketing, embeddings_roberta)
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_roberta.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
model_name = "roberta"
visualize_bertopic(topic_model_roberta, model_name)
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
# create a sentence model using the "all-mpnet-base-v2" pre-trained model
sentence_model = SentenceTransformer("gtr-t5-large", device='cuda')
# encode the 'docs_marketing' text data into embeddings
# and show a progress bar while processing
embeddings_gtr = sentence_model.encode(docs_marketing, show_progress_bar=True)
# create an instance of BERToopic with vectorizer_model and other parameters
topic_model_gtr = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
# adjust BERTopic to embeddings. It extracts topics and corresponding probabilities.
topics_gtr, probs_gtr = topic_model_gtr.fit_transform(docs_marketing, embeddings_roberta)
#update topic
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_gtr.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
model_name = "gtr"
visualize_bertopic(topic_model_gtr, model_name)
def generate_topics_table(topic_model):
# get topic information from the model
topics_info = topic_model.get_topic_info()
# convert the data into a list
data_as_list = topics_info.values.tolist()
# get column names as headers
headers = topics_info.columns.tolist()
# generate the table in HTML format
table = tabulate(data_as_list, headers, tablefmt='html')
# save the table to an HTML file
with open('table_topics.html', 'w') as f:
f.write(table)
def visualize_bertopic(topic_model, model_name):
# create the "images" folder if it doesn't exist already
if not os.path.exists("images"):
os.makedirs("images")
# create a subfolder for the specific topic model
model_folder = os.path.join("images", model_name)
# Create the model folder if it doesn't exist
if not os.path.exists(model_folder):
os.makedirs(model_folder)
else:
# Delete existing files in the model folder
for file in os.listdir(model_folder):
os.remove(os.path.join(model_folder, file))
# generate topics information table
generate_topics_table(topic_model)
os.rename('table_topics.html', os.path.join(model_folder, 'table_topics.html'))
# visualize topics
fig_topics = topic_model.visualize_topics()
fig_topics.write_html(os.path.join(model_folder, "topicsinfo.html"))
# visualize hierarchy
fig_hierarchy = topic_model.visualize_hierarchy()
fig_hierarchy.write_html(os.path.join(model_folder, "hierarchy.html"))
# visualize hierarchical topics
hierarchical_topics = topic_model.hierarchical_topics(docs_marketing)
fig_hierarchical_topics = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchical_topics.write_html(os.path.join(model_folder, "hierarchical.html"))
# visualize the bar chart
fig_barchart = topic_model.visualize_barchart(width=300, height=300, n_words=10, topics=None, top_n_topics=20)
fig_barchart.write_html(os.path.join(model_folder, "barchart.html"))
# visualize the heatmap
fig_heatmap = topic_model.visualize_heatmap()
fig_heatmap.write_html(os.path.join(model_folder, "heatmap.html"))
# topics over time
years =  df['year'].to_list()
topics_over_time = topic_model.topics_over_time(docs_marketing, years)
fig_topics_over_time = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, normalize_frequency=True)
fig_topics_over_time.write_html(os.path.join(model_folder, "topicsovertime.html"))
# create a countvectorizer object with specified settings:
# - ngram_range=(1, 3): generate both unigrams, bigrams and trigrams from the text.
#   this means it will consider individual words as well as pairs of consecutive words.
# - stop_words="english": use a predefined list of English stop words to exclude common words like "the," "and," etc.
#vectorizer_model = CountVectorizer(ngram_range=(1, 3)) #, stop_words="english"
# reduce the impact of frequent words
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
# create a sentence model using the "all-mpnet-base-v2" pre-trained model
sentence_model = SentenceTransformer("all-mpnet-base-v2", device='cuda')
# encode the 'docs_marketing' text data into embeddings
# and show a progress bar while processing
embeddings_mpnet = sentence_model.encode(docs_marketing, show_progress_bar=True)
# create an instance of BERToopic with vectorizer_model and other parameters
topic_model_mpnet = BERTopic(ctfidf_model=ctfidf_model, calculate_probabilities=True, verbose=True, min_topic_size=5, nr_topics=13)
# adjust BERTopic to embeddings. It extracts topics and corresponding probabilities.
topics_mpnet, probs_mpnet = topic_model_mpnet.fit_transform(docs_marketing, embeddings_mpnet)
#update topic
updated_vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 3))
topic_model_mpnet.update_topics(docs_marketing, vectorizer_model=updated_vectorizer_model)
#for sentence, embedding in zip(docs_marketing, embeddings_mpnet):
#print("Sentence:", sentence)
#print("Embedding:", embedding)
#print("")
model_name = "all-mpnet-base-v2"
visualize_bertopic(topic_model_mpnet, model_name)
