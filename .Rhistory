install.packages("reactablefmtr")
install.packages("gt")
#| label: load-packages
#| message: false
library(tidyverse)
library(word2vec)
#| label: load-packages
#| message: false
library(tidyverse)
library(word2vec)
library(quanteda)
library(text)
library(udpipe)
library(text2vec)
#| label: load-data
list_articles <- read.csv2("nlp_full_data_final_18-08-2023.csv", encoding = "UTF-8") %>%
rename("entry_number" = 1)
list_references <- read.csv2("nlp_references_final_18-08-2023.csv", encoding = "UTF-8") %>%
rename("citing_art" = 1)
colnames(list_articles) <- gsub("\\.+", "_", colnames(list_articles)) # <1>
colnames(list_articles) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_articles)) # <2>
colnames(list_references) <- gsub("\\.+", "_", colnames(list_references))
colnames(list_references) <- gsub("^[[:punct:]]+|[[:punct:]]+$", "", colnames(list_references))
data_embeddings <- list_articles %>%
distinct(entry_number, .keep_all = TRUE) %>%
filter(marketing == 1) %>%
mutate("combined_text" = paste0(dc_title,". ", dc_description)) %>%
mutate("year" = substr(prism_coverDate,7,10))
#| label: glimpse-data
data_embeddings %>%
head(5) %>%
select(entry_number, dc_creator, combined_text, year)
#| label: word2vec
set.seed(42)
model <- word2vec(x = tolower(data_embeddings$combined_text), type = "skip-gram", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(model)
lookslike <- predict(model, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike
lookslike %>%
ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
geom_point()
lookslike %>%
ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
#| label: word2vec
set.seed(42)
modelcbow <- word2vec(x = tolower(data_embeddings$combined_text), type = "cbow", dim = 100, iter = 100,, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(modelcbow)
#embedding <- predict(modelcbow, c("mining"), type = "embedding")
#embedding
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike
lookslike %>%
ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
geom_point()
lookslike %>%
ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike
lookslike %>%
ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
geom_point()
lookslike %>%
ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
t1=Sys.time()
UD <- udpipe_annotate(udmodel_french, x=data_embeddings$combined_text, trace =10000, parallel.cores = 6)
fr <- udpipe_download_model(language = "english")
udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
udmodel_english <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
t1=Sys.time()
t1=Sys.time()
UD <- udpipe_annotate(udmodel_english, x=data_embeddings$combined_text, trace =10000, parallel.cores = 6)
Sys.time()-t1
annotated_text <- UD %>% as.data.frame()
View(annotated_text)
modelcbow <- word2vec(x = tolower(annotated_text %>% filter(upos == "NOUN") %>% select(lemma)), type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
nouns <- annotated_text %>%
filter(upos == "NOUN") %>%
select(lemma)
View(nouns)
modelcbow <- word2vec(x = nouns, type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
modelcbow <- word2vec(x = nouns$lemma type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
modelcbow <- word2vec(x = nouns$lemma, type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(modelcbow)
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike
lookslike %>%
ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
geom_point()
lookslike %>%
ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
lemma <- annotated_text %>%
select(lemma)
modelcbow <- word2vec(x = lemma$lemma, type = "cbow", dim = 100, iter = 100, verbose=10, stopwords = stopwords("english"), window = "7")
embedding <- as.matrix(modelcbow)
lookslike <- predict(modelcbow, c("text"), type = "nearest", top_n = 20) %>% as.data.frame()
lookslike
lookslike %>%
ggplot(aes(x=text.similarity,y=reorder(text.term2,lookslike$text.similarity)))+
geom_point()
lookslike %>%
ggplot(aes(x = text.similarity, y = reorder(text.term2, text.similarity))) +
geom_point(color = "royalblue", size = lookslike$text.similarity+2) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
lemma <- annotated_text %>%
filter(upos == "NOUN") %>%
summarize(n=n())
View(lemma)
lemma <- annotated_text %>%
filter(upos == "NOUN") %>%
group_by(lemma) %>%
summarize(n=n())
lemma %>%
ggplot(aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue", size = n) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
lemma %>%
ggplot(aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue", size = n) +
theme_minimal() +
labs(x = "similarity score",
y = "") +
ggtitle("Top 20 similarity scores with term 'text'" ) +
theme(plot.title = element_text(hjust = 0.5))
lemma <- annotated_text %>%
filter(upos == "NOUN") %>%
group_by(lemma) %>%
summarize(n = n())
ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue", size = n) +
theme_minimal() +
labs(x = "Frequency",
y = "Lemma") +
ggtitle("Top 20 Most Frequent Nouns") +
theme(plot.title = element_text(hjust = 0.5))
lemma <- annotated_text %>%
filter(upos == "NOUN") %>%
group_by(lemma) %>%
summarize(n = n())
ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue") +
theme_minimal() +
labs(x = "Frequency",
y = "Lemma") +
ggtitle("Top 20 Most Frequent Nouns") +
theme(plot.title = element_text(hjust = 0.5))
lemma <- annotated_text %>%
filter(upos == "NOUN") %>%
group_by(lemma) %>%
summarize(n = n()) %>%
top_n(20)
ggplot(lemma, aes(x = n, y = reorder(lemma, n))) +
geom_point(color = "royalblue") +
theme_minimal() +
labs(x = "Frequency",
y = "Lemma") +
ggtitle("Top 20 Most Frequent Nouns") +
theme(plot.title = element_text(hjust = 0.5))
reticulate::repl_python()
from trankit import Pipeline
# initialize a pipeline for English
p = Pipeline('english')
# a non-empty string to process, which can be a document or a paragraph with multiple sentences
doc_text = '''Hello! This is Trankit.'''
all = p.posdep(doc_text)
doc_text = '''Hello! This is Trankit.'''
all = p.posdep(doc_text)
p = Pipeline('english')
p = Pipeline('english')
# a non-empty string to process, which can be a document or a paragraph with multiple sentences
# a non-empty string to process, which can be a document or a paragraph with multiple sentences
doc_text = '''Hello! This is Trankit.'''
all = p.posdep(doc_text)
from trankit import Pipeline
# initialize a pipeline for English
p = Pipeline('english')
# a non-empty string to process, which can be a document or a paragraph with multiple sentences
doc_text = '''Hello! This is Trankit.'''
all = p.posdep(doc_text)
all = p.posdep(doc_text)
from trankit import Pipeline
# initialize a pipeline for English
p = Pipeline('english')
# a non-empty string to process, which can be a document or a paragraph with multiple sentences
doc_text = '''Hello! This is Trankit.'''
all = p.posdep(doc_text)
from trankit import Pipeline
p = Pipeline(lang='english', gpu=True, cache_dir='./cache')
quit
tet <- "bonjour je m'appelle Olivier et je veux tester ce qu'est un corpus avec quanteda"
corpus <- corpus(tet)
corpus
tokens <- tokens(tet)
tokens
reticulate::repl_python()
import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
doc = nlp('Barack Obama was born in Hawaii.')
print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
!pip install stanza
import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
doc = nlp('Barack Obama was born in Hawaii.')
print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
print(doc)
import panda as pd
!pip install panda
!pip install pandas
import pandas as pd
