---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "A focus on authors, articles, references with networks"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Purpose

```{r}
#| label: introduction

cowsay::say("After researching the articles and references by making graphs to
better visualize the structure of the research. We want to focus
here on the authors, trying to understand how communities evolve over time.")
```

## Libraries and preparing data

```{python}
#| label: load-libraries-python
#| echo: false

#Libraries
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from itertools import combinations

#Data
data = pd.read_csv("data_final.csv")
list_articles = pd.read_csv("nlp_full_data_final_18-08-2023.csv", sep=';', decimal=',')
list_articles = list_articles[list_articles['marketing'] == 1] #only marketing articles

# Merge the "topic" and "topic_name" columns from the "data" dataframe into "list_articles"
list_articles = list_articles.merge(data[['entry_number', 'topic', 'topic_name']], on='entry_number', how='left')

list_references = pd.read_csv("nlp_references_final_18-08-2023.csv", sep=';', decimal=',')
```

### Check name of authors

We need to check if there are more than one unique `authorname` per `authid`. If so, we need to change the different names of author to the same name in order to have the exact same node per author later in the network.

```{r}
#| label: load-libraries-r

library(tidyverse)
library(reactable)

list_articles <- read_csv2("nlp_full_data_final_18-08-2023.csv")

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE)

result %>% reactable()

number_duplicates <- nrow(result)

cat("There are ", number_duplicates, " authors registered with different names.")
```

### Correct the duplicate names

Let's correct that by using one property of the distinct function: the `.keep_all = TRUE` parameter. It keeps the first occurrence of each group, which is the first row encountered for each unique combination of `authid` and `authname`. It will be faster than manually changing the name of each author.

```{r}
#| label: change-data-authors

# Merge list_articles with result on the authid column
merged_df <- left_join(list_articles, result, by = "authid")

# Replace authname values in list_articles with those from result
list_articles$authname <- ifelse(!is.na(merged_df$authname.y), merged_df$authname.y, list_articles$authname)

# Write the updated dataframe to a CSV file
write_csv2(list_articles, "nlp_full_data_final_unique_author_names.csv")

```

It is now done. We can check again if there are more than one unique `authorname` per `authid`.

### Verification of duplicate names

```{r}
#| label: check-unique-name-authors

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE) %>%
  relocate(entry_number)

result %>% reactable()
```

It's alright, we can now continue on constructing the network.

## Construct the dataframe for the network

```{python}
#| label: network-authors

# Define the year ranges
year_ranges = [(None, 2013), (2013, 2017), (2018, 2021), (2022, 2023)]

# Initialize a list to store the results for each year period
result_dfs = []

# Iterate through the year ranges
for start_year, end_year in year_ranges:
    if start_year is None:
        # Filter articles before 2013
        filtered_articles = list_articles[list_articles['year'] < end_year]
    else:
        # Filter articles within the specified year range
        filtered_articles = list_articles[(list_articles['year'] >= start_year) & (list_articles['year'] <= end_year)]

    # Create a list to store author pairs and their details for the current year period
    author_pairs = []

    # Group the filtered dataframe by article number and collect unique author IDs for each article
    grouped = filtered_articles.groupby('entry_number')[['authid', 'authname']].agg(list).reset_index()

    # Iterate through the grouped dataframe and find author pairs for each article
    for _, row in grouped.iterrows():
        entry_number = row['entry_number']
        authors = row['authid']
        authnames = row['authname']

        if len(authors) == 1:
            # Handle single authors by creating a self-relation
            author_pairs.append((entry_number, authors[0], authors[0], authnames[0], authnames[0]))
        elif len(authors) > 1:
            # Create pairs of authors who have co-authored the article
            author_combinations = list(combinations(range(len(authors)), 2))
            for i, j in author_combinations:
                author_pairs.append((entry_number, authors[i], authors[j], authnames[i], authnames[j]))

    # Create the DataFrame with the additional 'entry_number' column for the current year period
    result_df = pd.DataFrame(author_pairs, columns=['entry_number', 'authid1', 'authid2', 'authname1', 'authname2'])

    # Append the result DataFrame to the list of results
    result_dfs.append(result_df)

# Now, result_dfs contains DataFrames for each year period
# result_dfs[0] corresponds to articles before 2013
# result_dfs[1] corresponds to articles from 2013 to 2017
# result_dfs[2] corresponds to articles from 2018 to 2021
# result_dfs[3] corresponds to articles from 2022 to 2023

authors_before_2013 = result_dfs[0]
authors_2013_2017 = result_dfs[1]
authors_2018_2021 = result_dfs[2]
authors_2022_2023 = result_dfs[3]

```

```{python}
#| label: function-calculate-edge-weights

# Sort the cases with a->b and b->a and sum them up => it creates weighted edges
def get_collaboration_df(df):
    collaboration_df = df[["authname1","authname2"]]
    collaboration_df = pd.DataFrame(np.sort(collaboration_df.values, axis=1), columns=collaboration_df.columns)

    collaboration_df['value'] = 1
    collaboration_df = collaboration_df.groupby(["authname1","authname2"], sort=False, as_index=False).sum()
    return collaboration_df
  

# Example usage:
# Apply the function to one of your result DataFrames (e.g., result_dfs[0])
network_data_2022_2023 = get_collaboration_df(authors_2022_2023)
network_data_2022_2023.to_csv("network_data_2022_2023.csv")
```

## Create the network

```{python}
#| label: network-creation

G = nx.from_pandas_edgelist(network_data_2022_2023, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())


```

### Network basic visualization

```{python}
#| label: network-visualization

pos = nx.kamada_kawai_layout(G)

nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)
```

### Network visualization with pyvis

```{python}
#| label: network-visualization-pyvis
from pyvis.network import Network


net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
#net.show_buttons(filter_=['physics'])
net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 50
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

node_degree = dict(G.degree)

## Some values for nodes
# Multiply node sizes by two
node_degree_doubled = {node: 2 * degree for node, degree in node_degree.items()}
node_degree_centrality = nx.degree_centrality(G)
node_degree_betweenness = nx.betweenness_centrality(G)
node_degree_closeness = nx.closeness_centrality(G)


# Set the node attributes with the doubled sizes
nx.set_node_attributes(G, node_degree_doubled, 'size')
nx.set_node_attributes(G, node_degree_centrality, 'centrality')
nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
nx.set_node_attributes(G, node_degree_closeness, 'closeness')

#listnodes = net.nodes
#net.nodes.__getitem__(1)

#listnodes = net.nodes

net.from_nx(G)

net.show
net.show("networks/network_2022_2023.html")

```
```{=html}
<iframe width="1500" height="900" src="networks/network_2022_2023.html" title="Quarto Documentation" frameborder=0 class="column-page"></iframe>
```


#### Detect communities with Louvain's algorithm

```{python}
#using community_louvain from networkx

import community as community_louvain

# Compute the best partition
communities = community_louvain.best_partition(G)
communities

nx.set_node_attributes(G, communities, 'group')

```

```{python}
com_net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
com_net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 50
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

com_net.from_nx(G)
com_net.show("networks/network_2022_2023_louvain.html")
```
```{=html}
<iframe width="1500" height="900" src="networks/network_2022_2023_louvain.html" title="Quarto Documentation" frameborder=0 class="column-page"></iframe>
```


### Network visualization with ipysigma

```{python}
#| label: ipysigma


from ipysigma import Sigma
# Using Sigma from ipysigma
Sigma.write_html(G,
                 node_metrics={"community": "louvain"},
                 node_color="community",
                 path="networks/2022_2023_sigma.html",
                 fullscreen=True,
                 start_layout=3,
                 default_edge_type="curve",
                 node_border_color_from="node",
                 node_label_size=G.degree,
                 label_font="Gallica"
                 )
                 
```

```{=html}
<iframe width="1400" height="800" src="networks/2022_2023_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```



