---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "A focus on authors, articles, references with networks"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 10
format:
  html:
    theme:
      light: yeti
      #dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Purpose

```{r}
#| label: introduction

cowsay::say("After researching the articles and references by making graphs to
better visualize the structure of the research. We want to focus
here on the authors, trying to understand how communities evolve over time.")
```

## Libraries and preparing data

```{python}
#| label: load-libraries-python
#| echo: false

#Libraries
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import plotly.express as px
import re

from ipysigma import Sigma, SigmaGrid
from itertools import combinations
from datetime import datetime

#Data
data = pd.read_csv("data_final.csv")
data.rename(columns={'citedby-count': 'citedby_count'}, inplace=True)
list_articles = pd.read_csv("nlp_full_data_final_18-08-2023.csv", sep=';', decimal=',')
list_articles = list_articles[list_articles['marketing'] == 1] #only marketing articles

# Merge the "topic" and "topic_name" columns from the "data" dataframe into "list_articles"
# We do that to have the topic of each article in the list_articles dataframe
list_articles = list_articles.merge(data[['entry_number', 'topic', 'topic_name']], on='entry_number', how='left')

list_references = pd.read_csv("nlp_references_final_18-08-2023.csv", sep=';', decimal=',')
```

### Summary of the authors data

```{r}
#| label: summary-authors-data
#| column: screen-inset-right
#| echo: fenced


library(tidyverse)
library(reactable)
library(gt)
library(skimr)
library(plotly)
library(reticulate)
library(patchwork)



list_articles <- read_csv2("nlp_full_data_final_18-08-2023.csv") %>%
  mutate(marketing = as.logical(marketing)) %>%
  mutate(authid = as.character(authid)) %>%
  mutate(afid = as.character(afid)) %>%
  mutate(entry_number = as.character(entry_number)) %>%
  mutate(source_id = as.character(source_id)) %>%
  mutate(article_number = as.character(article_number)) %>%
  mutate(openaccess = as.logical(openaccess))

skim(list_articles) #%>%
  #filter(!skim_type %in% c("logical"))
```

### Check name of authors

We need to check if there are more than one unique `authorname` per `authid`. If so, we need to change the different names of author to the same name in order to have the exact same node per author later in the network.

```{r}
#| label: load-libraries-r

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE)

result %>% reactable()

number_duplicates <- nrow(result)

cat("There are ", number_duplicates, " authors registered with different names.")
```

### Correct the duplicate names

Let's correct that by using one property of the distinct function: the `.keep_all = TRUE` parameter. It keeps the first occurrence of each group, which is the first row encountered for each unique combination of `authid` and `authname`. It will be faster than manually changing the name of each author.

```{r}
#| label: change-data-authors

# Merge list_articles with result on the authid column
merged_df <- left_join(list_articles, result, by = "authid")

# Replace authname values in list_articles with those from result
list_articles$authname <- ifelse(!is.na(merged_df$authname.y), merged_df$authname.y, list_articles$authname)

# Keep only marketing articles and filter "Erratum" type of publications (=correction)
list_articles <- list_articles %>% 
  filter(marketing == 1) %>%
  filter(subtypeDescription != "Erratum")


cat("There are", n_distinct(list_articles$entry_number), "articles and", n_distinct(list_articles$authname), "authors overall in the data.")

# Write the updated dataframe to a CSV file 
write_csv2(list_articles, "nlp_full_data_final_unique_author_names.csv")

```

It is now done. We can check again if there are more than one unique `authorname` per `authid`.

### Verification of duplicate names

```{r}
#| label: check-unique-name-authors

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE) %>%
  relocate(entry_number)

result %>% reactable()
```

It's alright, we can now continue on constructing the data frames for the networks.

## Construct the dataframes for the networks

### Create the dataframes of collaboration between authors (one per period)

```{python}
#| label: network-authors

list_articles =  pd.read_csv("nlp_full_data_final_unique_author_names.csv", sep=';', decimal=',')
# Define the year ranges
year_ranges = [(None, 2013), (2013, 2017), (2018, 2021), (2022, 2023)]

# Initialize a list to store the results for each year period
result_dfs = []

# Iterate through the year ranges
for start_year, end_year in year_ranges:
    if start_year is None:
        # Filter articles before 2013
        filtered_articles = list_articles[list_articles['year'] < end_year]
    else:
        # Filter articles within the specified year range
        filtered_articles = list_articles[(list_articles['year'] >= start_year) & (list_articles['year'] <= end_year)]

    # Create a list to store author pairs and their details for the current year period
    author_pairs = []

    # Group the filtered dataframe by article number and collect unique author IDs for each article
    grouped = filtered_articles.groupby('entry_number')[['authid', 'authname']].agg(list).reset_index()

    # Iterate through the grouped dataframe and find author pairs for each article
    for _, row in grouped.iterrows():
        entry_number = row['entry_number']
        authors = row['authid']
        authnames = row['authname']

        if len(authors) == 1:
            # Handle single authors by creating a self-relation
            author_pairs.append((entry_number, authors[0], authors[0], authnames[0], authnames[0]))
        elif len(authors) > 1:
            # Create pairs of authors who have co-authored the article
            author_combinations = list(combinations(range(len(authors)), 2))
            for i, j in author_combinations:
                author_pairs.append((entry_number, authors[i], authors[j], authnames[i], authnames[j]))

    # Create the DataFrame with the additional 'entry_number' column for the current year period
    result_df = pd.DataFrame(author_pairs, columns=['entry_number', 'authid1', 'authid2', 'authname1', 'authname2'])

    # Append the result DataFrame to the list of results
    result_dfs.append(result_df)

# Now, result_dfs contains DataFrames for each year period
# result_dfs[0] corresponds to articles before 2013
# result_dfs[1] corresponds to articles from 2013 to 2017
# result_dfs[2] corresponds to articles from 2018 to 2021
# result_dfs[3] corresponds to articles from 2022 to 2023

authors_before_2013 = result_dfs[0]
authors_2013_2017 = result_dfs[1]
authors_2018_2021 = result_dfs[2]
authors_2022_2023 = result_dfs[3]

```

### Sort cases with a-\>b and b-\>a and create weighted edges

```{python}
#| label: function-calculate-edge-weights

# Sort the cases with a->b and b->a and sum them up => it creates weighted edges
def get_collaboration_df(df):
    collaboration_df = df[["authname1","authname2"]]
    collaboration_df = pd.DataFrame(np.sort(collaboration_df.values, axis=1), columns=collaboration_df.columns)

    collaboration_df['value'] = 1
    collaboration_df = collaboration_df.groupby(["authname1","authname2"], sort=False, as_index=False).sum()
    return collaboration_df
  

# Apply the function to DataFrames
network_data_2022_2023 = get_collaboration_df(authors_2022_2023)
network_data_2022_2023.to_csv("networks/csv/network_data_2022_2023.csv")
network_data_2018_2021 = get_collaboration_df(authors_2018_2021)
network_data_2018_2021.to_csv("networks/csv/network_data_2018_2021.csv")
network_data_2013_2017 = get_collaboration_df(authors_2013_2017)
network_data_2013_2017.to_csv("networks/csv/network_data_2013_2017.csv")
network_data_before_2013 = get_collaboration_df(authors_before_2013)
network_data_before_2013.to_csv("networks/csv/network_data_before_2013.csv")
```

### Get information about authors

```{python}
#| label: get_author_info_by_period


def sort_dict(dict):
    sorted_dict = {k: v for k, v in sorted(dict.items(), key=lambda item: item[0])}
    return sorted_dict
  
def get_author_info_by_period(list_articles, period):
    # Filter the DataFrame based on the specified period
    if period == "before-2013":
        filtered_df = list_articles[list_articles['year'] < 2013]
    elif period == "2013-2017":
        filtered_df = list_articles[(list_articles['year'] >= 2013) & (list_articles['year'] <= 2017)]
    elif period == "2018-2021":
        filtered_df = list_articles[(list_articles['year'] >= 2018) & (list_articles['year'] <= 2021)]
    elif period == "2022-2023":
        filtered_df = list_articles[(list_articles['year'] >= 2022) & (list_articles['year'] <= 2023)]
    else:
        raise ValueError("Invalid period parameter. Please choose one of: 'before-2013', '2013-2017', '2018-2021', '2022-2023")
    
    # Create a dictionary to store affiliations and countries for each author
    info_author = {'affiliation': {}, 'country': {}, 'article': {}, 'journal': {}}

    # Iterate through the filtered DataFrame to collect affiliations and countries for each author
    for index, row in filtered_df.iterrows():
        author_name = row['authname']
        affiliation = row['affilname']
        country = row['affiliation_country']
        article = row['dc:title']
        journal = row['prism:publicationName']
        
        # Check if the affiliation is not NaN (i.e., a valid affiliation)
        if isinstance(affiliation, str) and affiliation.strip() != "":
            # Check if the author already has an affiliation, and if the new one is not already included, then add it with "&"
            if author_name in info_author['affiliation']:
                if affiliation not in info_author['affiliation'][author_name]:
                    info_author['affiliation'][author_name] += " | " + affiliation
            else:
                info_author['affiliation'][author_name] = affiliation
        
        # Check if the country is not NaN (i.e., a valid country)
        if isinstance(country, str) and country.strip() != "":
            # Check if the author already has a country, and if the new one is not already included, then add it with "&"
            if author_name in info_author['country']:
                if country not in info_author['country'][author_name]:
                    info_author['country'][author_name] += " | " + country
            else:
                info_author['country'][author_name] = country
                
        # Check if the article title is not NaN (i.e., a valid article title)
        if isinstance(article, str) and article.strip() != "":
            # Check if an author has multiple articles, if so add the new article with "&"
            if author_name in info_author['article']:
                if article not in info_author['article'][author_name]:
                    info_author['article'][author_name] += " | " + article
            else:
                info_author['article'][author_name] = article
                
        # Check if the journal name (or conference name) is not NaN (i.e., a valid journal)
        if isinstance(journal, str) and journal.strip() != "":
            # Check if the author has published in multiple journals, if so add the new journal with "&"
            if author_name in info_author['journal']:
                if journal not in info_author['journal'][author_name]:
                    info_author['journal'][author_name] += " | " + journal
            else:
                info_author['journal'][author_name] = journal

    # Create a dictionary that associates each author with their ln(total number of citations+1)+1
    # We do +1 twice because if the result is 0, then the log is undefined.
    # We add another +1 to avoid having 0 values in the network.
    citedby_dict = filtered_df.groupby('authname')['citedby_count'].sum() .to_dict()
    #citedby_dict = filtered_df.groupby('authname')['citedby_count'].sum().apply(lambda x: np.log(x + 1) + 1).to_dict()

    # Add the 'citedby_count' dictionary to the 'info_author' dictionary under the key 'citations'
    info_author['citations'] = citedby_dict

    # Return the info_author dictionary containing both affiliation, country, and citations information
    return info_author


  
# How to use:
# Specify the period of interest: ("before-2013", "2013-2017", "2018-2021", or "2022-2023")
# The function returns a dictionary with four keys: 'affiliation', 'country', 'article', 'journal', and 'citations'
# The dictionaries are then used as attributes for the nodes in the network (see below)

affilauthor_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['affiliation'])
affilauthor_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['affiliation'])
affilauthor_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['affiliation'])
affilauthor_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['affiliation'])
countryauthor_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['country'])
countryauthor_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['country'])
countryauthor_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['country'])
countryauthor_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['country'])
citations_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['citations'])
citations_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['citations'])
citations_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['citations'])
citations_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['citations'])
article_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['article'])
article_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['article'])
article_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['article'])
article_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['article'])
journal_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['journal'])
journal_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['journal'])
journal_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['journal'])
journal_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['journal'])

```

## Co-authorship networks

```{python}
#| label: network-creation

G = nx.from_pandas_edgelist(network_data_2022_2023, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())

```

### Network basic visualization

This is a basic visualization done with the NetworkX library and matplotlib.

```{python}
#| label: network-visualization
#| echo: fenced
#| fig-cap: Visualization of the co-authorship network for the 2022-2023 period, created using NetworkX and matplotlib.
#| fig-align: center

plt.figure(figsize=(20,20))
pos = nx.kamada_kawai_layout(G)

nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)
```

::: callout-caution
This is not interactive and the result is not enlightening at all. We then decide to use `Pyvis` to create an interactive visualization.
:::

### Network visualization with Pyvis

::: callout-tip
Pyvis enables us to create interactive visualizations and modify the network layout in real time with the `net.show_buttons(filter_=['physics'])` command. This button then generates options to include in our code by using the `net.set_options()` function. [More information here](https://pyvis.readthedocs.io/en/latest/tutorial.html#using-the-configuration-ui-to-dynamically-tweak-network-settings "button physics Pyvis").
:::

```{python}
#| label: network-visualization-pyvis
#| output: false
#| echo: fenced

from pyvis.network import Network


net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
#net.show_buttons(filter_=['physics'])
net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 70
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

node_degree = dict(G.degree)

## Some values for nodes
# Multiply node sizes by two
node_degree_doubled = {node: 2 * degree for node, degree in node_degree.items()}
node_degree_centrality = nx.degree_centrality(G)
node_degree_betweenness = nx.betweenness_centrality(G)
node_degree_closeness = nx.closeness_centrality(G)
node_degree_constraint = nx.constraint(G)


# Set the node attributes with the doubled sizes
nx.set_node_attributes(G, node_degree_doubled, 'size')
nx.set_node_attributes(G, node_degree_centrality, 'centrality')
nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
nx.set_node_attributes(G, node_degree_closeness, 'closeness')
nx.set_node_attributes(G, node_degree_constraint, 'constraint')
nx.set_node_attributes(G, affilauthor_2022_2023, 'affiliation')
nx.set_node_attributes(G, countryauthor_2022_2023, 'country')
nx.set_node_attributes(G, citations_2022_2023, 'citations')
nx.set_node_attributes(G, article_2022_2023, 'title')
nx.set_node_attributes(G, journal_2022_2023, 'journal')

#listnodes = net.nodes
#net.nodes.__getitem__(1)

#listnodes = net.nodes

net.from_nx(G)

net.show("networks/authors/network_2022_2023_pyvis.html")

```

```{=html}
<iframe width="1500" height="900" src="networks/authors/network_2022_2023_pyvis.html" title="Quarto Documentation" frameborder=0 class="column-page"></iframe>
```
### Detect communities with Louvain's algorithm

```{python}
#| label: community-detection-louvain
#| output: false
#| echo: fenced

import community as community_louvain

# Compute the best partition
communities = community_louvain.best_partition(G)

#dftest = pd.DataFrame(list(communities.items()), columns=['authname', 'community'])

nx.set_node_attributes(G, communities, 'group')

```

```{python}
#| label: network-visualization-pyvis-louvain
#| output: false
#| echo: fenced
com_net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
com_net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 50
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

com_net.from_nx(G)
com_net.show("networks/authors/network_2022_2023_louvain_pyvis.html")
```

```{=html}
<iframe width="1500" height="900" src="networks/authors/network_2022_2023_louvain_pyvis.html" title="network_2022_2023_louvain" frameborder=0 class="column-page"></iframe>
```
### Network visualization with ipysigma [@plique:hal-03903518v1]

#### A function to graph them all

::: column-margin
![](images/graph_ring.png){fig-align="center" width="289"}
:::

```{python}
#| label: function-ipysigma-network

def sigma_graph(dataframe, year_period):
    # Create a graph from the given dataframe
    G = nx.from_pandas_edgelist(dataframe, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())

    # Set edge colors for visualization
    for u, v in G.edges:
        G[u][v]["color"] = "#7D7C7C"

    # Calculate the degree of each node
    node_degree = dict(G.degree)

    # Compute multiple centrality metrics for nodes
    node_degree_centrality = nx.degree_centrality(G)
    node_degree_betweenness = nx.betweenness_centrality(G)
    node_degree_closeness = nx.closeness_centrality(G)
    node_degree_eigenvector = nx.closeness_centrality(G)
    node_degree_constraint_weighted = nx.constraint(G, weight="value")
    node_degree_constraint_unweighted = nx.constraint(G)
    
    # Set node attributes for various metrics
    nx.set_node_attributes(G, node_degree_centrality, 'centrality')
    nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
    nx.set_node_attributes(G, node_degree_closeness, 'closeness')
    nx.set_node_attributes(G, node_degree_eigenvector, 'eigenvector centrality')
    nx.set_node_attributes(G, node_degree_constraint_weighted, 'burt\'s constraint weighted')
    nx.set_node_attributes(G, node_degree_constraint_unweighted, 'burt\'s constraint unweighted')
    

    # Mapping of variables based on the provided 'year_period'
    affilauthor_mapping = {
        "before-2013": affilauthor_before_2013,
        "2013-2017": affilauthor_2013_2017,
        "2018-2021": affilauthor_2018_2021,
        "2022-2023": affilauthor_2022_2023
    }

    countryauthor_mapping = {
        "before-2013": countryauthor_before_2013,
        "2013-2017": countryauthor_2013_2017,
        "2018-2021": countryauthor_2018_2021,
        "2022-2023": countryauthor_2022_2023
    }

    citations_mapping = {
        "before-2013": citations_before_2013,
        "2013-2017": citations_2013_2017,
        "2018-2021": citations_2018_2021,
        "2022-2023": citations_2022_2023
    }
    
    article_mapping = {
        "before-2013": article_before_2013,
        "2013-2017": article_2013_2017,
        "2018-2021": article_2018_2021,
        "2022-2023": article_2022_2023
    }
    
    journal_mapping = {
        "before-2013": journal_before_2013,
        "2013-2017": journal_2013_2017,
        "2018-2021": journal_2018_2021,
        "2022-2023": journal_2022_2023
    }

    # Set node attributes based on the selected 'year_period'
    # They appear in the graph in the same order they are added
    nx.set_node_attributes(G, affilauthor_mapping[year_period], 'affiliation')
    nx.set_node_attributes(G, countryauthor_mapping[year_period], 'country')
    nx.set_node_attributes(G, article_mapping[year_period], 'articles')
    nx.set_node_attributes(G, journal_mapping[year_period], 'journals')
    nx.set_node_attributes(G, citations_mapping[year_period], 'citations')
    nx.set_node_attributes(G, node_degree_constraint_weighted, 'burt\'s constraint weighted')
    nx.set_node_attributes(G, node_degree_constraint_unweighted, 'burt\'s constraint unweighted')

    # Construct the sigma graph and customize visualization
    Sigma.write_html(G,
                 default_edge_type       = "curve",                                                # Default edge type
                 clickable_edges         = True,                                                   # Clickable edges
                 edge_size               = "value",                                                # Set edge size
                 fullscreen              = True,                                                   # Display in fullscreen
                 label_density           = 2,                                                      # Label density
                 label_font              = "Helvetica Neue",                                       # Label font
                 max_categorical_colors  = 10,                                                     # Max categorical colors
                 node_border_color_from  = 'node',                                                 # Node border color from node attribute
                 node_color              = "community",                                            # Set node colors
                 node_label_size         = "citations",                                            # Node label size
                 node_label_size_range   = (12, 36),                                               # Node label size range
                 node_metrics            = {"community": {"name": "louvain", "resolution": 1}},    # Specify node metrics
                 node_size               = "citations",                                            # Node size
                 node_size_range         = (3, 30),                                                # Node size range
                 path                    = f"networks/authors/{year_period}_sigma.html",           # Output file path
                 start_layout            = 3                                                       # Start layout algorithm
                 #node_border_color      = "black",                                                # Node border color
                 #edge_color             = "source",                                               # Edge color
                 # node_label_color      = "community"                                             # Node label color
                 )

    return G


```

#### Co-authorship network for the 2022-2023 period ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/authors/2022-2023_sigma.html))

```{python}
#| label: network-visualization-ipysigma-2022-2023

import random
import community as community_louvain

# Fix the seed for reproducibility
random.seed(42)

G_2022_2023 = sigma_graph(network_data_2022_2023, '2022-2023')

#detect how many commmunities there are in the graph
communities = community_louvain.best_partition(G_2022_2023)
print("There are {} communities in the 2022-2023 network".format(len(set(communities.values()))))
#print("The density of the graph is {}".format(round(nx.density(G_2022_2023), 6)))

test_value = nx.constraint(G_2022_2023, nodes=None, weight="value")
test_novalue = nx.constraint(G_2022_2023, nodes=None, weight=None)
```

```{=html}
<iframe width="1500" height="900" src="networks/authors/2022-2023_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
#### Co-authorship network for the 2018-2021 period ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/authors/2018-2021_sigma.html))

```{python}
#| label: network-visualization-ipysigma-2018-2021

G_2018_2021 = sigma_graph(network_data_2018_2021, '2018-2021')

#print("The density of the graph is {}".format(nx.density(G_2018_2021))

communities = community_louvain.best_partition(G_2018_2021)
print("There are {} communities in the 2018-2021 network".format(len(set(communities.values()))))

```

```{=html}
<iframe width="1500" height="900" src="networks/authors/2018-2021_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
#### Co-authorship network for the 2013-2017 period ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/authors/2013-2017_sigma.html))

```{python}
#| label: network-visualization-ipysigma-2013-2017

G_2013_2017 = sigma_graph(network_data_2013_2017, '2013-2017')

#print("The density of the graph is {}".format(nx.density(G_2013_2017))

communities = community_louvain.best_partition(G_2013_2017)
print("There are {} communities in the 2013-2017 network".format(len(set(communities.values()))))
```

```{=html}
<iframe width="1500" height="900" src="networks/authors/2013-2017_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
#### Co-authorship network before 2013 ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/authors/before-2013_sigma.html))

```{python}
#| label: network-visualization-ipysigma-before-2013

G_before_2013 = sigma_graph(network_data_before_2013, 'before-2013')

communities = community_louvain.best_partition(G_before_2013)
print("There are {} communities in the before-2013 network".format(len(set(communities.values()))))
```

```{=html}
<iframe width="1500" height="900" src="networks/authors/before-2013_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
### An interesting metric: the graph density

The graph density is the ratio of the number of edges to the maximum number of possible edges. It is a measure of the proportion of edges present in a graph. A graph with a high density has a large number of edges compared to the number of nodes. A graph with a low density has a small number of edges compared to the number of nodes.

A more formal definition is given [here](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.density.html) by the following formulas:

-   For undirected graphs:

$$
\begin{equation}d=\frac{2 m}{n(n-1)}\end{equation}
$$

-   For directed graphs:

$$
\begin{equation}d=\frac{2 m}{n(n-1)}\end{equation}
$$

where $n$ is the number of nodes and $m$ is the number of edges in the graph.

From an interpretation standpoint, we can appreciate the density in the graphs bellow as follows:

+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| $d$          | **Interpretation**                                                                                                                                         |
+:============:+:===========================================================================================================================================================+
| Close to $0$ | -   The collaborative relationships among authors are sparse:                                                                                              |
|              |                                                                                                                                                            |
|              | -   Authors have limited connections with each other outside of their community.                                                                           |
|              |                                                                                                                                                            |
|              | -   Scientific papers are primarily the work of individual authors or small isolated groups.                                                               |
+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Close to $1$ | -   Authors frequently collaborate with one another, leading to a web of interconnected scientific collaborations.                                         |
|              |                                                                                                                                                            |
|              | -   Scientific papers often involve contributions from multiple authors, reflecting a high level of teamwork and interdisciplinary research.               |
|              |                                                                                                                                                            |
|              | -   Collaborations are a significant aspect of the research process in this marketing field, and authors actively seek out opportunities to work together. |
|              |                                                                                                                                                            |
|              | -   The network of collaborations is well-established and robust, facilitating the exchange of ideas and the advancement of scientific knowledge.          |
+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+

#### Evolution of the graphs' density

```{python}
#| label: evolution-graphs-density
#| output: false
def average_degree(G):
    # Calculate the sum of degrees of all nodes
    total_degree = sum(dict(G.degree()).values())
    
    # Divide by the number of nodes to get the average degree
    avg_degree = total_degree / G.number_of_nodes()
    
    return avg_degree
  
def linear_density(G):
    if len(G.nodes()) == 0:  # Pour éviter une division par zéro
        return 0
    return len(G.edges()) / len(G.nodes())
  
# Create a dataframe with the density of each graph
density_df = pd.DataFrame({
    'period': ['before-2013', '2013-2017', '2018-2021', '2022-2023'],
    'density': [
      nx.density(G_before_2013),
      nx.density(G_2013_2017),
      nx.density(G_2018_2021),
      nx.density(G_2022_2023)
      ],
    'average_degree': [
        average_degree(G_before_2013),
        average_degree(G_2013_2017),
        average_degree(G_2018_2021),
        average_degree(G_2022_2023)
    ],
    'linear_density': [
        linear_density(G_before_2013),
        linear_density(G_2013_2017),
        linear_density(G_2018_2021),
        linear_density(G_2022_2023)
    ]
})
```

```{r}
#| label: evolution-graphs-density-r
library(Hmisc)

# Load the 'density_df' dataframe from Python using reticulate
testtransfer <- py$density_df

# Specify the order of categories for the 'period' column
testtransfer$period <- factor(testtransfer$period, levels = c('before-2013', '2013-2017', '2018-2021', '2022-2023'))

# Use the 'gt()' function to display the dataframe
testtransfer %>%
  rename_all(Hmisc::capitalize) %>%
  gt() %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels()
    )
        
 
# Create the Plotly graph
fig <- plot_ly(testtransfer, x = ~period, y = ~density, type = 'scatter', mode = 'lines+markers', 
               text = ~paste("Period=", period, "<br>Density=", density), hoverinfo = "text")

# Show the graph
fig <- fig %>% layout(template = "plotly_white")

fig
```

## Citations networks

Let's now dive into the exploration of citation networks. We'll be employing the same approach that we used for analyzing co-authorship networks across different time periods.

In this new investigation, our primary aim remains the acquisition of valuable insights into the ever-evolving landscape of research in marketing using NLP methods.

This research is motivated by the convergence of two critical factors:

1.  The advent of novel tools and techniques that facilitate the analysis of large data volumes ;

2.  The proliferation and the availability of open and private data from various sectors.

While our prior work focused on uncovering emerging research topics, our current focus is about comprehending which papers have garnered the most attention. We seek to determine whether it is predominantly computer science papers that have inspired marketing scholars with new perspectives into data analysis or if marketing papers have also played a role in advocating the development of new theories.

### Data preparation and summary

We'll start by loading the data of references and preparing it for the analysis.

```{r}
#| label: citations-data-preparation

# Load the data of references
list_references <- read_csv2('nlp_references_final_18-08-2023.csv')

# Get the current year
current_year <- as.integer(format(Sys.Date(), "%Y"))


# Perform the following operations on the list_references DataFrame:
# 1. Select the first 32 columns
# 2. Extract the relevant part of the 'citing_art' column
# 3. Rename columns for easier reference
# 4. Reorder the 'scopus_id' column
# 5. Extract the year from the 'prism:coverDate' column
# 6. Calculate the 'citations_per_year' column
# 7. Round the 'citations_per_year' column to two decimal places
# 8. Remove the original 'prism:coverDate' column

list_references <- list_references %>%
  select(1:32) %>%
  mutate(citing_art = substr(citing_art, 11, nchar(citing_art))) %>%
  rename(author = `author-list.author.ce:indexed-name`,
         scopus_id = `scopus-id`,
         citedby_count = `citedby-count`) %>%
  relocate(scopus_id, .after = citing_art) %>%
  mutate(year = as.integer(substr(`prism:coverDate`, 1, 4))) %>%
  mutate(citations_per_year = ifelse(!is.na(citedby_count) & !is.na(year), 
                                     citedby_count / (current_year - year + 1), 
                                     NA)) %>%
  mutate(citations_per_year = round(citations_per_year, 2)) %>%
  mutate(year = as.character(year)) %>%
  select(-`prism:coverDate`)

write_csv2(list_references, 'list_ref_test_to_delete.csv')
```

```{python}
skim(list_references)
```

#### Detect inconsistencies

There are some problems with the data that we need to address before proceeding with the analysis.

Some `scopus_id` identifiers (the id of the reference that appears in our marketing NLP corpus articles) have multiple different values of title (even only minor differences), sourcetitle, etc. although they should be equal.

We want to plot the networks with information about the nodes but we need to have only one unique value for each variable of `scopus_id`.

```{python}
#| label: citations-detect-inconsistencies

list_references = r.list_references

# Group by 'scopus_id' and count the unique number of 'title' for each 'scopus_id'
title_counts = list_references.groupby('scopus_id')['title'].nunique()

# Find the 'scopus_id' that have more than one associated title
inconsistent_scopus_id = title_counts[title_counts > 1].index.tolist()
```

#### List of all inconsistencies

```{r}
#| label: citations-show-inconsistencies

list_inconsistencies <- list_references %>%
  filter(scopus_id %in% py$inconsistent_scopus_id) %>% #we take the inconsistent scopus_id from python by using reticulate
  select(scopus_id, citing_art, title, sourcetitle, year, author)


reactable(
  list_inconsistencies,
  striped = TRUE,
  groupBy = "scopus_id",
  defaultPageSize = 5,
  defaultColDef = colDef(minWidth = 100, maxWidth = 200),  # Adjust these values as needed
  columns = list(
    title = colDef(minWidth = 250)  # Adjust this value based on the length of your titles
  )
)
```

#### Correct inconsistencies

To correct the inconsistencies, we'll use the `standardize_values()` function. Specifically:

1.  **Grouping by Unique Identifiers**: We first organize data by a unique identifier like `scopus_id`. This collates all relevant entries for a particular article or reference.

2.  **Standardizing Titles and Source Names**: Next, within each group, we harmonize key values such as article titles, source names, and author names to remove variations.

3.  **Standardization Priorities**: To choose the 'standard' value within each group, we apply a set of rules that favor the most commonly occurring value, using additional tie-breaker criteria as needed (number of characters, capitalized letters).

```{python}
#| label: citations-correct-inconsistencies

def standardize_values(df, groupby_column, value_column):
    """
    Standardize the values of the specified column based on the most frequent non-empty value and fewest characters 
    within each group.

    Parameters:
    - df: DataFrame
    - groupby_column: The column by which we group data.
    - value_column: The column whose values we want to standardize based on the rules.

    Returns:
    - DataFrame with standardized values.
    """
    
    def custom_mode(series):
        # Remove NA values and other representations of NA
        series = series.dropna()
        series = series[~series.isin(['', 'NA'])]
        
        # If all values were NA or empty
        if series.empty:
            return np.nan  # Using numpy's nan for consistency

        # Get value counts
        counts = series.value_counts()

        # If there's a single most common value, return it
        if len(counts) == 1 or counts.iloc[0] != counts.iloc[1]:
            return counts.idxmax()

        # If multiple values have the same max count, apply further rules
        top_values = counts[counts == counts.iloc[0]].index.tolist()

        # Sort by fewest characters
        sorted_by_chars = sorted(top_values, key=lambda x: len(x))

        # If there's a single value with the fewest characters, return it
        if len(sorted_by_chars) == 1 or len(sorted_by_chars[0]) != len(sorted_by_chars[1]):
            return sorted_by_chars[0]

        # If the column is not the author's name, apply the uppercase letter rule.
        if value_column != "author_name":  # adjust "author_name" to the correct column name if necessary
            return sorted(sorted_by_chars, key=lambda x: sum(1 for c in x if c.isupper()), reverse=True)[0]
        else:
            return sorted_by_chars[0]

    # Find the most common value for each group based on the custom mode
    most_common_value = df.groupby(groupby_column)[value_column].apply(custom_mode).to_dict()

    # Map the most common values to the dataframe based on the group
    df[value_column] = df[groupby_column].map(most_common_value)

    return df


# Usage example:
list_references_standardized = standardize_values(list_references, 'scopus_id', 'title')
list_references_standardized = standardize_values(list_references_standardized, 'scopus_id', 'sourcetitle')
list_references_standardized = standardize_values(list_references_standardized, 'scopus_id', 'author')

```

#### Check inconsistencies

We check that the inconsistencies have been corrected.

```{r}
#| label: citations-check-inconsistencies

check_inconsistencies <- py$list_references_standardized %>%
  filter(scopus_id %in% py$inconsistent_scopus_id) %>% #we take the inconsistent scopus_id from python by using reticulate
  select(scopus_id, citing_art, title, sourcetitle, year, author)


reactable(
  check_inconsistencies,
  striped = TRUE,
  defaultPageSize = 5,
  groupBy = "scopus_id",
  defaultColDef = colDef(minWidth = 100, maxWidth = 200), 
  columns = list(
    title = colDef(minWidth = 250)
  )
)

```

#### Get missing data (to be done later if necessary)

We have a lot of missing data when it comes to the year of publication of the articles, the title, the sourcetitle the authors, DOI. It could be helpful to retrieve these data so we can see them when we click on the nodes.

```{r}
#| label: citations-get-missing-data-todo

# First, let's construct a df where "year" is missing::
missing_years <- py$list_references_standardized %>%
  filter(year == "NA") %>%
  select(scopus_id, citing_art, title, sourcetitle, year, author, `ce:doi`)


```

## Construct the dataframes

### Construct the dataframe for the network of references

```{python}
#| label: citations-construct-dataframes

def get_citations_df(df, start_year=None, end_year=None):
    """
    Filter and extract necessary columns for citation network from a DataFrame based on a range of years.
    
    Parameters:
    - df: DataFrame containing the data
    - start_year: Optional, the starting year for filtering
    - end_year: Optional, the ending year for filtering
    
    Returns:
    - DataFrame with filtered data
    """
    
    # Replace 'NA' with numpy.nan and reassign the DataFrame
    df = df.replace({'year': 'NA'}, np.nan)
    
    # Drop rows where 'year' is NaN
    df = df.dropna(subset=['year'])
    
    # Convert the 'year' column to integer using .astype
    df['year'] = df['year'].astype(int)
    
    # Filter the data based on the 'year' column only if start_year and end_year are provided
    if start_year is not None and end_year is not None:
        df = df[df['year'].between(start_year, end_year)]
    
    # Extract necessary columns for the citation network
    citations_df = df[['citing_art', 'scopus_id', 'sourcetitle', 'title', 'citedby_count', 'citations_per_year' , 'author', 'year']]
    
    return citations_df

# Using the function to get a->b standardized data for the citations networks below
citations_df_2022_2023 = get_citations_df(list_references_standardized, 2022, 2023)
citations_df_2018_2021 = get_citations_df(list_references_standardized, 2018, 2021)
citations_df_2013_2017 = get_citations_df(list_references_standardized, 2013, 2017)
citations_df_before_2013 = get_citations_df(list_references_standardized, 0, 2012)
citations_df_overall = get_citations_df(list_references_standardized)  #No filter on years: 

```

### Add information about references and citing articles to the dataframes

Ipysigma allows us to view different information when clicking on the nodes. This enhances the interactive experience by providing context-relevant details for each node in the network.

To achieve this functionality, we have two primary options:

-   The first is to create a loop that assigns the relevant information, such as author name, year, title, and source title, to each individual node.

-   The second option is to construct a dictionary where each node serves as a key and the corresponding information serves as the value.

We opted for the latter approach. This dictionary is then passed as attributes to the nodes using NetworkX's **`set_node_attributes`** property.

```{python}
#| label: citations-add-information

def get_info_references_dict(df, key, column):
    """
    Create a dictionary with keys from the specified key_column and values from the specified value_column.

    :param df: Input DataFrame.
    :param key_column: Column name to be used as keys in the resulting dictionary.
    :param value_column: Column name to be used as values in the resulting dictionary.
    :return: Dictionary with keys from key_column and values from value_column.
    """
    if key not in df.columns or column not in df.columns:
        raise ValueError("The required columns are not present in the DataFrame.")
    return sort_dict(df.set_index(key)[column].to_dict())
  

# We also need to get the info of the citing articles, otherwise we won't get any info when we click 
# on the nodes and we will have the number of the node as node label instead of the author name
# Create the 'citing_art' column by stripping the first 10 characters from 'dc_identifier'
data['citing_art'] = data['dc_identifier'].str[10:]

# Getting the current year
current_year = datetime.now().year

# English Comment: Function to calculate citations per year, handles NaN values, division by zero, and the current year.
def calculate_citations_per_year(row):
    if pd.isna(row['year']):
        return np.nan
    elif (current_year - row['year']) == 0:
        return 0  # Handle division by zero by returning 0
    else:
        return round(row['citedby_count'] / (current_year - row['year']), 2)

# Creating the new column 'citations_per_year'
data['citations_per_year'] = data.apply(calculate_citations_per_year, axis=1)

# List of columns to use in the networks later as attributes. We can add more columns if we want to.
columns_to_extract = ['title', 'sourcetitle', 'citedby_count', 'author', 'year', 'citations_per_year']

# Dictionary of dataframes with their respective names
dfs = {
    "2022_2023": citations_df_2022_2023,
    "2018_2021": citations_df_2018_2021,
    "2013_2017": citations_df_2013_2017,
    "before_2013": citations_df_before_2013,
    "overall": citations_df_overall
}


# Map old column names to new column names
column_mapping = {
    'title': 'dc_title',
    'sourcetitle': 'prism_publicationName',
    'author': 'dc_creator',
    'year': 'year',
    'citations': 'citedby_count',
    'citations_per_year': 'citations_per_year'
}

# Reverse mapping for merging
reverse_column_mapping = {v: k for k, v in column_mapping.items()}

# Rename columns in data DataFrame for merging
data.rename(columns=reverse_column_mapping, inplace=True)


# Initialize the output dictionary
dict_references = {}

# Retrieve the information for each period and each column
for period, df in dfs.items():
    dict_references[period] = {}
    for column in columns_to_extract:
        # This check is important in case all columns are not present across all dataframes
        if column in df.columns:
            dict_references[period][column] = get_info_references_dict(df, 'scopus_id', column)

    # Get the citing_art dictionary from 'data' DataFrame
    for column in columns_to_extract:
        if column in data.columns:
            citing_art_dict = get_info_references_dict(data, 'citing_art', column)
            
            # Add to dict_references only if key is not already present 
            # It means that the article in our data has been cited by others and is then already present in the references dataframe
            for key, value in citing_art_dict.items():
                if key not in dict_references[period].get(column, {}):
                    dict_references[period].setdefault(column, {})[key] = value


#print(dict_references) 

# Just to check if the key is in the dictionary
#if "85149566147" in dict_references['2022_2023']['author']:
    #print("The key '85149566147' exists.")
#else:
    #print("The key '85149566147' does not exist.")

```

## Construct the networks with ipysigma

### Another function to graph them all

```{python}
#| label: citations-construct-networks

def sigma_graph_references(dataframe, period_label):
        
    # Create a graph from the given dataframe
    G = nx.from_pandas_edgelist(dataframe, 'citing_art', 'scopus_id', create_using=nx.DiGraph())
    
    # Fetch attributes for the given period from the global dict_references
    attributes_dict = dict_references.get(period_label, {})

    # Set the attributes from dict_references to the nodes of the graph
    for attribute, attribute_dict in attributes_dict.items():
        nx.set_node_attributes(G, attribute_dict, name=attribute)

    # Set edge colors for visualization
    for u, v in G.edges:
        G[u][v]["color"] = "#7D7C7C"

    # Calculate the degree of each node
    node_degree = dict(G.degree)

    # Compute multiple centrality metrics for nodes
    node_degree_centrality = nx.degree_centrality(G)
    node_degree_betweenness = nx.betweenness_centrality(G)
    node_degree_closeness = nx.closeness_centrality(G)
    node_degree_eigenvector = nx.closeness_centrality(G)
    #node_degree_constraint_weighted = nx.constraint(G, weight="value")
    node_degree_constraint_unweighted = nx.constraint(G)
    
    # Set node attributes for various metrics
    nx.set_node_attributes(G, node_degree_centrality, 'centrality')
    nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
    nx.set_node_attributes(G, node_degree_closeness, 'closeness')
    nx.set_node_attributes(G, node_degree_eigenvector, 'eigenvector centrality')
    #nx.set_node_attributes(G, node_degree_constraint_weighted, 'burt\'s constraint weighted')
    nx.set_node_attributes(G, node_degree_constraint_unweighted, 'burt constraint unweighted')
    
    # Set node attributes based on the selected 'year_period'
    

    # Construct the sigma graph and customize visualization
    Sigma.write_html(G,
                 default_edge_type      = "arrow",                                                # Set default edge type
                 fullscreen             = True,                                                   # Display in fullscreen mode
                 label_density          = 2,                                                      # Increase this to have more labels appear
                 label_font             = "Helvetica Neue",                                       # Set label font
                 max_categorical_colors = 30,                                                     # Max categorical colors for communities
                 node_border_color_from = 'node',                                                 # Set node border color from 'node' attribute
                 node_color             = "community",                                            # Set node colors
                 node_label             = "author",                                               # Set node label from 'author' attribute
                 node_label_size        = G.in_degree,                                            # Set node label size
                 node_label_size_range  = (12, 36),                                               # Set node label size range
                 node_metrics           = {"community": {"name": "louvain", "resolution": 1}},    # Specify node metrics
                 node_size              = G.in_degree,                                            # Set node size based on the in_degree attribute
                 node_size_range        = (3, 30),                                                # Set node size range
                 path                   = f"networks/references/{period_label}_sigma.html",       # Specify the output file path
                 start_layout           = 10                                                       # Start the layout algorithm automatically and lasts 5 seconds
                 #node_border_color     = "black",                                                # Set node border color
                 #edge_color            = "source",                                               # Set edge color from 'source' attribute
                 )

    return G

```

### Citations network for 2022-2023 ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/references/2022_2023_sigma.html))

```{python}
#| label: citations-construct-network-2022-2023

G_2022_2023_references = sigma_graph_references(citations_df_2022_2023, "2022_2023")

#burt_constraint_unweighted = nx.get_node_attributes(G_2022_2023_references, 'burt constraint unweighted')
#df_burt_constraint = pd.DataFrame(list(burt_constraint_unweighted.items()), columns=['Node', 'Burt_Constraint_Unweighted'])
#df_sorted = df_burt_constraint.sort_values(by='Burt_Constraint_Unweighted')
#print(df_sorted)

SigmaGrid.to_html(
    G_2022_2023_references,
    views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
    node_size_range=(2, 10),
    default_edge_type='curve',
    path="networks/references/2022_2023_sigma_grid.html",
)

SigmaGrid(G_2022_2023_references,
    views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
    node_size_range=(2, 10),
    default_edge_type='curve',
    #path="networks/references/2022_2023_sigma_grid.html"
    )
```

```{=html}
<iframe width="1500" height="900" src="networks/references/2022_2023_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
### Citations network for 2018-2021 ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/references/2018_2021_sigma.html))

```{python}
#| label: citations-construct-network-2018-2021

G_2018_2021_references = sigma_graph_references(citations_df_2018_2021, "2018_2021")

```

```{=html}
<iframe width="1500" height="900" src="networks/references/2018_2021_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
### Citations network for 2013-2017 ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/references/2013_2017_sigma.html))

```{python}
#| label: citations-construct-network-2013-2017

G_2013_2017_references = sigma_graph_references(citations_df_2013_2017, "2013_2017")


```

```{=html}
<iframe width="1500" height="900" src="networks/references/2013_2017_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
### Citations network for before 2013 ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/references/before_2013_sigma.html))

```{python}
#| label: citations-construct-network-before-2013

G_before_2013_references = sigma_graph_references(citations_df_before_2013, "before_2013")

```

```{=html}
<iframe width="1500" height="900" src="networks/references/before_2013_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
### Citations network for overall ([click here for fullscreen](https://oliviercaron.github.io/systematic_lit_review/networks/references/overall_sigma.html))

```{python}
#| label: citations-construct-network-overall

G_overall_references = sigma_graph_references(citations_df_overall, "overall")

```

```{=html}
<iframe width="1500" height="900" src="networks/references/overall_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
## Graph density of references

```{python}
#| label: citations-graph-density
#| output: false



# Create a dataframe with the density of each graph
density_df_references = pd.DataFrame({
    'period': ['before-2013', '2013-2017', '2018-2021', '2022-2023', 'overall'],
    'density': [
        nx.density(G_before_2013_references), 
        nx.density(G_2013_2017_references), 
        nx.density(G_2018_2021_references), 
        nx.density(G_2022_2023_references),
        nx.density(G_overall_references)
    ],
    'average_degree': [
        average_degree(G_before_2013_references),
        average_degree(G_2013_2017_references),
        average_degree(G_2018_2021_references),
        average_degree(G_2022_2023_references),
        average_degree(G_overall_references)
    ],
    'linear_density': [
    linear_density(G_before_2013_references),
    linear_density(G_2013_2017_references),
    linear_density(G_2018_2021_references),
    linear_density(G_2022_2023_references),
    linear_density(G_overall_references)
    ]
})

```

```{r}
#| label: citations-graph-density-comparison
#| fig.cap: Comparison of network densities and average degree of nodes over time
#| column: body-outset

# Density plot
density_plot <- ggplot() +
  geom_line(data = py$density_df, aes(x = period, y = density, colour = "Collaboration Density", group=1, text = paste("Period:", period, "<br>Density:", density)), linewidth=1) +
  geom_line(data = py$density_df_references %>% filter(period != "overall"), aes(x = period, y = density, colour = "References Density", group=1, text = paste("Period:", period, "<br>Density:", density)), linewidth=1) +
  scale_y_continuous(name = "Graphs Density") +
  scale_x_discrete(limits = c("before-2013", "2013-2017", "2018-2021", "2022-2023")) +
  xlab("Period") +
  ggtitle("Comparison of Network Density Over Time") +
  theme_minimal()

# Linear density plot (m/n)
linear_density_plot <- ggplot() +
  geom_line(data = py$density_df, aes(x = period, y = linear_density, colour = "Collaboration Linear Density", group=1, text = paste("Period:", period, "<br>Linear Density:", linear_density)), linewidth=1) + # Adjusted for the new column "linear_density"
  geom_line(data = py$density_df_references %>% filter(period != "overall"), aes(x = period, y = linear_density, colour = "References Linear Density", group=1, text = paste("Period:", period, "<br>Linear Density:", linear_density)), linewidth=1) + # Adjusted for the new column "linear_density"
  scale_y_continuous(name = "Graphs Linear Density") +
  scale_x_discrete(limits = c("before-2013", "2013-2017", "2018-2021", "2022-2023")) +
  xlab("Period") +
  ggtitle("Comparison of Network Linear Density Over Time") +
  theme_minimal()

# Create average degree plot
avg_degree_plot <- ggplot() +
  geom_line(data = py$density_df, aes(x = period, y = average_degree, colour = "Collaboration Average Degree", group=1, text = paste("Period:", period, "<br>Average Degree:", average_degree)), linewidth=1) + # Corrected here
  geom_line(data = py$density_df_references %>% filter(period != "overall"), aes(x = period, y = average_degree, colour = "References Average Degree", group=1, text = paste("Period:", period, "<br>Average Degree:", average_degree)), linewidth=1) +
  scale_y_continuous(name = "Nodes Average Degree") +
  scale_x_discrete(limits = c("before-2013", "2013-2017", "2018-2021", "2022-2023")) +
  xlab("Period") +
  ggtitle("Comparison of Network Average Degree Over Time") +
  theme_minimal()

# Combine density and average degree plots
density_plot / linear_density_plot / avg_degree_plot

ggsave("images/citations-graph-density-comparison.png", width=270, height=180, units="cm", dpi=300)

```
