---
title: "Systematic literature review"
bibliography: references.bib
title-block-banner: true
subtitle: "A focus on authors, articles, references with networks"
author:
  - name: Olivier Caron
    email: olivier.caron@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    email: christophe.benavent@dauphine.psl.eu
    affiliations: 
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date : "last-modified"
toc: true
number-sections: true
number-depth: 5
format:
  html:
    theme:
      light: yeti
      dark: darkly
    code-fold: true
    code-summary: "Display code"
    code-tools: true #enables to display/hide all blocks of code
    code-copy: true #enables to copy code
    grid:
      body-width: 1000px
      margin-width: 100px
    toc: true
    toc-location: left
execute:
  echo: true
  warning: false
  message: false
editor: visual
fig-align: "center"
highlight-style: ayu
css: styles.css
reference-location: margin
---

## Purpose

```{r}
#| label: introduction

cowsay::say("After researching the articles and references by making graphs to
better visualize the structure of the research. We want to focus
here on the authors, trying to understand how communities evolve over time.")
```

## Libraries and preparing data

```{python}
#| label: load-libraries-python
#| echo: false

#Libraries
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from itertools import combinations

#Data
data = pd.read_csv("data_final.csv")
list_articles = pd.read_csv("nlp_full_data_final_18-08-2023.csv", sep=';', decimal=',')
list_articles = list_articles[list_articles['marketing'] == 1] #only marketing articles

# Merge the "topic" and "topic_name" columns from the "data" dataframe into "list_articles"
list_articles = list_articles.merge(data[['entry_number', 'topic', 'topic_name']], on='entry_number', how='left')

list_references = pd.read_csv("nlp_references_final_18-08-2023.csv", sep=';', decimal=',')
```

### Check name of authors

We need to check if there are more than one unique `authorname` per `authid`. If so, we need to change the different names of author to the same name in order to have the exact same node per author later in the network.

```{r}
#| label: load-libraries-r

library(tidyverse)
library(reactable)

list_articles <- read_csv2("nlp_full_data_final_18-08-2023.csv")

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE)

result %>% reactable()

number_duplicates <- nrow(result)

cat("There are ", number_duplicates, " authors registered with different names.")
```

### Correct the duplicate names

Let's correct that by using one property of the distinct function: the `.keep_all = TRUE` parameter. It keeps the first occurrence of each group, which is the first row encountered for each unique combination of `authid` and `authname`. It will be faster than manually changing the name of each author.

```{r}
#| label: change-data-authors

# Merge list_articles with result on the authid column
merged_df <- left_join(list_articles, result, by = "authid")

# Replace authname values in list_articles with those from result
list_articles$authname <- ifelse(!is.na(merged_df$authname.y), merged_df$authname.y, list_articles$authname)

# Write the updated dataframe to a CSV file
write_csv2(list_articles, "nlp_full_data_final_unique_author_names.csv")

```

It is now done. We can check again if there are more than one unique `authorname` per `authid`.

### Verification of duplicate names

```{r}
#| label: check-unique-name-authors

test <- list_articles %>%
  group_by(authid) %>%
  select(authid, authname, entry_number) %>%
  mutate(n = n())

result <- test %>%
  group_by(authid) %>%
  filter(n_distinct(authname) > 1) %>%
  distinct(authid, .keep_all = TRUE) %>%
  relocate(entry_number)

result %>% reactable()
```

It's alright, we can now continue on constructing the data frames for the networks.

## Construct the dataframes for the networks

### Create the dataframes of collaboration between authors (one per period)

```{python}
#| label: network-authors

list_articles =  pd.read_csv("nlp_full_data_final_unique_author_names.csv", sep=';', decimal=',')
# Define the year ranges
year_ranges = [(None, 2013), (2013, 2017), (2018, 2021), (2022, 2023)]

# Initialize a list to store the results for each year period
result_dfs = []

# Iterate through the year ranges
for start_year, end_year in year_ranges:
    if start_year is None:
        # Filter articles before 2013
        filtered_articles = list_articles[list_articles['year'] < end_year]
    else:
        # Filter articles within the specified year range
        filtered_articles = list_articles[(list_articles['year'] >= start_year) & (list_articles['year'] <= end_year)]

    # Create a list to store author pairs and their details for the current year period
    author_pairs = []

    # Group the filtered dataframe by article number and collect unique author IDs for each article
    grouped = filtered_articles.groupby('entry_number')[['authid', 'authname']].agg(list).reset_index()

    # Iterate through the grouped dataframe and find author pairs for each article
    for _, row in grouped.iterrows():
        entry_number = row['entry_number']
        authors = row['authid']
        authnames = row['authname']

        if len(authors) == 1:
            # Handle single authors by creating a self-relation
            author_pairs.append((entry_number, authors[0], authors[0], authnames[0], authnames[0]))
        elif len(authors) > 1:
            # Create pairs of authors who have co-authored the article
            author_combinations = list(combinations(range(len(authors)), 2))
            for i, j in author_combinations:
                author_pairs.append((entry_number, authors[i], authors[j], authnames[i], authnames[j]))

    # Create the DataFrame with the additional 'entry_number' column for the current year period
    result_df = pd.DataFrame(author_pairs, columns=['entry_number', 'authid1', 'authid2', 'authname1', 'authname2'])

    # Append the result DataFrame to the list of results
    result_dfs.append(result_df)

# Now, result_dfs contains DataFrames for each year period
# result_dfs[0] corresponds to articles before 2013
# result_dfs[1] corresponds to articles from 2013 to 2017
# result_dfs[2] corresponds to articles from 2018 to 2021
# result_dfs[3] corresponds to articles from 2022 to 2023

authors_before_2013 = result_dfs[0]
authors_2013_2017 = result_dfs[1]
authors_2018_2021 = result_dfs[2]
authors_2022_2023 = result_dfs[3]

```

### Sort cases with a-\>b and b-\>a create weighted edges

```{python}
#| label: function-calculate-edge-weights

# Sort the cases with a->b and b->a and sum them up => it creates weighted edges
def get_collaboration_df(df):
    collaboration_df = df[["authname1","authname2"]]
    collaboration_df = pd.DataFrame(np.sort(collaboration_df.values, axis=1), columns=collaboration_df.columns)

    collaboration_df['value'] = 1
    collaboration_df = collaboration_df.groupby(["authname1","authname2"], sort=False, as_index=False).sum()
    return collaboration_df
  

# Apply the function to DataFrames
network_data_2022_2023 = get_collaboration_df(authors_2022_2023)
network_data_2022_2023.to_csv("networks/csv/network_data_2022_2023.csv")
network_data_2018_2021 = get_collaboration_df(authors_2018_2021)
network_data_2018_2021.to_csv("networks/csv/network_data_2018_2021.csv")
network_data_2013_2017 = get_collaboration_df(authors_2013_2017)
network_data_2013_2017.to_csv("networks/csv/network_data_2013_2017.csv")
network_data_before_2013 = get_collaboration_df(authors_before_2013)
network_data_before_2013.to_csv("networks/csv/network_data_before_2013.csv")
```

### Get affiliation name, country and number of citations for each author

```{python}
#| label: affiliation-data-nodes


def sort_dict(dict):
    sorted_dict = {k: v for k, v in sorted(dict.items(), key=lambda item: item[0])}
    return sorted_dict
  
import pandas as pd
import numpy as np

# Supposons que vous ayez un DataFrame list_articles avec des colonnes 'authname' et 'citedby_count'
# 'authname' contient les noms des auteurs, et 'citedby_count' contient le nombre de citations

def get_author_info_by_period(list_articles, period):
    # Filter the DataFrame based on the specified period
    if period == "before-2013":
        filtered_df = list_articles[list_articles['year'] < 2013]
    elif period == "2013-2017":
        filtered_df = list_articles[(list_articles['year'] >= 2013) & (list_articles['year'] <= 2017)]
    elif period == "2018-2021":
        filtered_df = list_articles[(list_articles['year'] >= 2018) & (list_articles['year'] <= 2021)]
    elif period == "2022-2023":
        filtered_df = list_articles[(list_articles['year'] >= 2022) & (list_articles['year'] <= 2023)]
    else:
        raise ValueError("Invalid period parameter. Please choose one of: 'before-2013', '2013-2017', '2018-2021', '2022-2023'")
    
    # Create a dictionary to store affiliations and countries for each author
    info_author = {'affiliation': {}, 'country': {}}

    # Iterate through the filtered DataFrame to collect affiliations and countries for each author
    for index, row in filtered_df.iterrows():
        author_name = row['authname']
        affiliation = row['affilname']
        country = row['affiliation_country']
        
        # Check if the affiliation is not NaN (i.e., a valid affiliation)
        if isinstance(affiliation, str) and affiliation.strip() != "":
            # Check if the author already has an affiliation, and if the new one is not already included, then add it with "&"
            if author_name in info_author['affiliation']:
                if affiliation not in info_author['affiliation'][author_name]:
                    info_author['affiliation'][author_name] += " | " + affiliation
            else:
                info_author['affiliation'][author_name] = affiliation
        
        # Check if the country is not NaN (i.e., a valid country)
        if isinstance(country, str) and country.strip() != "":
            # Check if the author already has a country, and if the new one is not already included, then add it with "&"
            if author_name in info_author['country']:
                if country not in info_author['country'][author_name]:
                    info_author['country'][author_name] += " | " + country
            else:
                info_author['country'][author_name] = country

    # Create a dictionary that associates each author with their ln(total number of citations+1)+1
    # We do +1 twice because if the result is 0, then the log is undefined.
    # We add another +1 to avoid having 0 values in the network.
    citedby_dict = filtered_df.groupby('authname')['citedby_count'].sum() .to_dict()
    #citedby_dict = filtered_df.groupby('authname')['citedby_count'].sum().apply(lambda x: np.log(x + 1) + 1).to_dict()


    # Add the 'citedby_count' dictionary to the 'info_author' dictionary under the key 'citations'
    info_author['citations'] = citedby_dict

    # Return the info_author dictionary containing both affiliation, country, and citations information
    return info_author

# How to use:
# Specify the period of interest: ("before-2013", "2013-2017", "2018-2021", or "2022-2023")
# The function returns a dictionary with two keys: 'affiliation' and 'country'
affilauthor_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['affiliation'])
affilauthor_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['affiliation'])
affilauthor_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['affiliation'])
affilauthor_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['affiliation'])
countryauthor_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['country'])
countryauthor_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['country'])
countryauthor_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['country'])
countryauthor_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['country'])
citations_before_2013 = sort_dict(get_author_info_by_period(list_articles, "before-2013")['citations'])
citations_2013_2017 = sort_dict(get_author_info_by_period(list_articles, "2013-2017")['citations'])
citations_2018_2021 = sort_dict(get_author_info_by_period(list_articles, "2018-2021")['citations'])
citations_2022_2023 = sort_dict(get_author_info_by_period(list_articles, "2022-2023")['citations'])

```

## Co-authorship networks

```{python}
#| label: network-creation

G = nx.from_pandas_edgelist(network_data_2022_2023, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())

```

### Network basic visualization

This is a basic visualization done with the NetworkX library and matplotlib.

```{python}
#| label: network-visualization
#| echo: fenced
#| fig-cap: Visualization of the co-authorship network for the 2022-2023 period, created using NetworkX and matplotlib.
#| fig-align: center

plt.figure(figsize=(20,20))
pos = nx.kamada_kawai_layout(G)

nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)
```

::: callout-caution
This is not interactive and the result is not enlightening at all. We then decide to use `Pyvis` to create an interactive visualization.
:::

### Network visualization with Pyvis

::: callout-tip
Pyvis enables us to create interactive visualizations and modify the network layout in real time with the `net.show_buttons(filter_=['physics'])` command. This button then generates options to include in our code by using the `net.set_options()` function. [More information here](https://pyvis.readthedocs.io/en/latest/tutorial.html#using-the-configuration-ui-to-dynamically-tweak-network-settings "button physics Pyvis").
:::

```{python}
#| label: network-visualization-pyvis
#| output: false
#| echo: fenced

from pyvis.network import Network


net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
#net.show_buttons(filter_=['physics'])
net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 70
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

node_degree = dict(G.degree)

## Some values for nodes
# Multiply node sizes by two
node_degree_doubled = {node: 2 * degree for node, degree in node_degree.items()}
node_degree_centrality = nx.degree_centrality(G)
node_degree_betweenness = nx.betweenness_centrality(G)
node_degree_closeness = nx.closeness_centrality(G)


# Set the node attributes with the doubled sizes
nx.set_node_attributes(G, node_degree_doubled, 'size')
nx.set_node_attributes(G, node_degree_centrality, 'centrality')
nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
nx.set_node_attributes(G, node_degree_closeness, 'closeness')
nx.set_node_attributes(G, affilauthor_2022_2023, 'affiliation')
nx.set_node_attributes(G, countryauthor_2022_2023, 'country')
nx.set_node_attributes(G, citations_2022_2023, 'citations')

#listnodes = net.nodes
#net.nodes.__getitem__(1)

#listnodes = net.nodes

net.from_nx(G)

net.show("networks/network_2022_2023.html")

```

```{=html}
<iframe width="1500" height="900" src="networks/network_2022_2023.html" title="Quarto Documentation" frameborder=0 class="column-page"></iframe>
```
### Detect communities with Louvain's algorithm

```{python}
#| label: community-detection-louvain
#| output: false
#| echo: fenced

import community as community_louvain

# Compute the best partition
communities = community_louvain.best_partition(G)

#dftest = pd.DataFrame(list(communities.items()), columns=['authname', 'community'])

nx.set_node_attributes(G, communities, 'group')

```

```{python}
#| label: network-visualization-pyvis-louvain
#| output: false
#| echo: fenced
com_net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor="white", font_color="black")
com_net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -13,
      "centralGravity": 0.015,
      "springLength": 50
    },
    "minVelocity": 0.75,
    "solver": "forceAtlas2Based"
  }
}
""")

com_net.from_nx(G)
com_net.show("networks/network_2022_2023_louvain.html")
```

```{=html}
<iframe width="1500" height="900" src="networks/network_2022_2023_louvain.html" title="network_2022_2023_louvain" frameborder=0 class="column-page"></iframe>
```
### Network visualization with ipysigma (@plique2022ipysigma)

#### A function to graph them all 

```{python}
#| label: function-ipysigma-network

def sigma_graph(dataframe, year_period):
    # Create a graph from the given dataframe
    G = nx.from_pandas_edgelist(dataframe, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())

    # Set edge colors for visualization
    for u, v in G.edges:
        G[u][v]["color"] = "#7D7C7C"

    # Calculate the degree of each node
    node_degree = dict(G.degree)

    # Compute multiple centrality metrics for nodes
    node_degree_centrality = nx.degree_centrality(G)
    node_degree_betweenness = nx.betweenness_centrality(G)
    node_degree_closeness = nx.closeness_centrality(G)
    
    # Set node attributes for various metrics
    nx.set_node_attributes(G, node_degree_centrality, 'centrality')
    nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
    nx.set_node_attributes(G, node_degree_closeness, 'closeness')

    # Mapping of variables based on the provided 'year_period'
    affilauthor_mapping = {
        "before-2013": affilauthor_before_2013,
        "2013-2017": affilauthor_2013_2017,
        "2018-2021": affilauthor_2018_2021,
        "2022-2023": affilauthor_2022_2023
    }

    countryauthor_mapping = {
        "before-2013": countryauthor_before_2013,
        "2013-2017": countryauthor_2013_2017,
        "2018-2021": countryauthor_2018_2021,
        "2022-2023": countryauthor_2022_2023
    }

    citations_mapping = {
        "before-2013": citations_before_2013,
        "2013-2017": citations_2013_2017,
        "2018-2021": citations_2018_2021,
        "2022-2023": citations_2022_2023
    }

    # Set node attributes based on the selected 'year_period'
    nx.set_node_attributes(G, affilauthor_mapping[year_period], 'affiliation')
    nx.set_node_attributes(G, countryauthor_mapping[year_period], 'country')
    nx.set_node_attributes(G, citations_mapping[year_period], 'citations')

    # Construct the sigma graph and customize visualization
    Sigma.write_html(G,
        node_metrics={"community": {"name": "louvain", "resolution": 1.5}},  # Specify node metrics
        edge_size="value",  # Set edge size
        node_color="community",  # Set node colors
        path=f"networks/{year_period}_sigma.html",  # Specify the output file path
        fullscreen=True,  # Display in fullscreen mode
        start_layout=1,  # Start the layout algorithm automatically and lasts 1 second
        max_categorical_colors=10,  # Max categorical colors for communities
        label_density=2,  # Increase this to have more labels appear, it is also possible to display all labels
        node_size_range=(3, 20),  # Set node size range
        default_edge_type="curve",  # Set default edge type
        label_font="Sans Serif",  # Set label font
        node_label_size="citations",  # Set node label size
        #node_border_color="black",  # Set node border color
        #edge_color="source",  # Set edge color from 'source' attribute
        node_border_color_from='node',  # Set node border color from 'node' attribute
        node_size="citations",  # Set node size based on 'citations' attribute
        node_label_size_range=(12, 25)
        # node_label_color="community"  # Set node label color based on 'community' attribute
    )

    return G


```

#### Co-authorship network for the 2022-2023 period

```{python}
#| label: network-visualization-ipysigma-2022-2023


G_2022_2023 = sigma_graph(network_data_2022_2023, '2022-2023')

                 
```

```{=html}
<iframe width="1500" height="900" src="networks/2022_2023_sigma.html" title="Sigma graph" frameborder=0 class="column-page"></iframe>
```
#### Co-authorship network for the 2018-2021 period

```{python}
#| label: network-visualization-ipysigma-2018-2021

G_2018_2021 = sigma_graph(network_data_2018_2021, '2018-2021')

```

#### Co-authorship network for the 2013-2017 period

```{python}
#| label: network-visualization-ipysigma-2013-2017

G_2013_2017 = sigma_graph(network_data_2013_2017, '2013-2017')


```

#### Co-authorship network before 2013

```{python}
#| label: network-visualization-ipysigma-before-2013

G_before_2013 = sigma_graph(network_data_before_2013, 'before-2013')


```

## Co-citations networks

Let's now dive into the exploration of co-citation networks. We'll be employing the same approach that we used for analyzing co-authorship networks across different time periods.

In this new investigation, our primary aim remains the acquisition of valuable insights into the ever-evolving landscape of research in marketing using text-mining (NLP) methods. This pursuit is motivated by the convergence of two critical factors: the advent of novel tools and techniques that facilitate the analysis of substantial data volumes and the proliferation of data from various sectors. With these considerations at the forefront, our central objective is to reveal the complex mechanisms that underlie the diffusion of research in marketing employing NLP methodologies. While our prior work focused on uncovering emerging research topics, our current emphasis shifts towards comprehending which papers have garnered the most attention. We seek to determine whether it's predominantly computer science papers that have inspired marketing scholars with fresh insights into data analysis or if marketing papers have also played a role in advocating the development of new theories.

### Data preparation

We'll start by loading the data of references and preparing it for the analysis.

```{python}
#| label: co-citation-data-preparation










```






















